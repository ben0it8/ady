{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ady-language-model-wordpiece.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben0it8/poetry-lm/blob/master/ady_language_model_wordpiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jpXivGD0YpHn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Check GPU memory availability"
      ]
    },
    {
      "metadata": {
        "id": "NVEc6D3Tlsnl",
        "colab_type": "code",
        "outputId": "207a8ec6-735c-4ba0-ea1d-99fad42f87b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 249.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Syp72_eY06e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Install extra requirements"
      ]
    },
    {
      "metadata": {
        "id": "EAGNkWy_D5er",
        "colab_type": "code",
        "outputId": "ac5a3517-1de8-445c-d9a1-3ce1c91b46e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U ftfy\n",
        "!pip install -U sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.5.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/8a/0e4a10bc00a0263db8d45d0062c83892598eb58e8091f439c63926e9b107/sentencepiece-0.1.81-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 18.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYlrr4nDEk5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'; # adapt plots for retina displays\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid');\n",
        "sns.set_context(context='notebook');\n",
        "from typing import List\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import numpy as np \n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import ftfy \n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import dill\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.nn import functional as F\n",
        "from random import sample\n",
        "\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "data_dir = Path(\"data/-wp\").resolve()\n",
        "data_dir.mkdir(exist_ok=True, parents=True)\n",
        "url = \"https://raw.githubusercontent.com/ben0it8/ady/master/data/arany.txt\" \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIaijzWl2G-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_paths(path):\n",
        "  path = Path(path)\n",
        "  ret = []\n",
        "  for x in path.glob('*'):\n",
        "    if x.is_file() and not str(x).endswith('.zip'):\n",
        "      ret.append(str(x))\n",
        "  return ret\n",
        "\n",
        "def zip_dir(path, name):\n",
        "  if not name.endswith('.zip'): name += '.zip'\n",
        "  file_paths = get_file_paths(path)\n",
        "  \n",
        "  with ZipFile(path/name, 'w') as zip:\n",
        "    for file in file_paths:\n",
        "      zip.write(file, os.path.basename(file))\n",
        "  print(f\"Zipped files at {path}\")\n",
        "  return (path/name).resolve()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIxEBl5Irzky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Read data"
      ]
    },
    {
      "metadata": {
        "id": "-HobfQaar-ID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean data & write to disk"
      ]
    },
    {
      "metadata": {
        "id": "LidRmCTeFDP-",
        "colab_type": "code",
        "outputId": "08da1926-f36b-4b11-afca-56aafd0a6ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "cell_type": "code",
      "source": [
        "def is_title(t):\n",
        "  t = re.sub('[\\d\\s\\s\\-!?,]', '', t)\n",
        "  return t.isupper()\n",
        "\n",
        "def fix_text(t:str):\n",
        "  t = ftfy.fix_text(t, normalization='NFKC')\n",
        "  t = t.replace('\\n', '') # remove newlines\n",
        "  t = re.sub(r'[»«]', '', t) # remove special parenthesis\n",
        "  t = re.sub(r'[0-9]','', t)\n",
        "  t = re.sub(\"\\s\\s+\", \" \", t) # skip whitespaces\n",
        "  t = t.strip()\n",
        "  return t\n",
        "    \n",
        "def fix_texts(texts:list):\n",
        "  out = []\n",
        "  for i, t in enumerate(texts):\n",
        "    t = fix_text(t)\n",
        "    if t is None or len(t)<=2 or is_title(t):\n",
        "      continue\n",
        "    else: out += [t]   \n",
        "  return out\n",
        "response =  requests.get(url)\n",
        "texts = [line.decode() for line in response.iter_lines()]\n",
        "print(f\"No. of lines: {len(texts)}\")\n",
        "clean_texts = fix_texts(texts)\n",
        "(data_dir/'arany_clean.txt').open(mode='wt').writelines(f\"{line}\\n\" for line in clean_texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of lines: 7787\n",
            "Halak a hálóban\n",
            "\n",
            "Te győzz le\n",
            "\n",
            "Te győzz le engem, éjszaka!\n",
            "\n",
            "Sötéten úszó és laza\n",
            "\n",
            "hullámaidba lépek.\n",
            "\n",
            "Tünődve benned görgetik\n",
            "\n",
            "fakó szivüknek terheit\n",
            "\n",
            "a hallgatag szegények\n",
            "\n",
            "A foszladó világ felett\n",
            "\n",
            "te változó és mégis egy,\n",
            "\n",
            "szelíd, örök vigasz vagy;\n",
            "\n",
            "elomlik minden kívüled,\n",
            "\n",
            "mit lágy erőszakod kivet,\n",
            "\n",
            "elomlik és kihamvad.\n",
            "\n",
            "De élsz te, s égve hirdetik\n",
            "\n",
            "hatalmad csillagképeid,\n",
            "\n",
            "ez ősi, néma ábrák:\n",
            "\n",
            "akár az első angyalok,\n",
            "\n",
            "belőled jöttem és vagyok,\n",
            "\n",
            "ragadj magadba, járj át!\n",
            "\n",
            "Feledd a hűtlenségemet,\n",
            "\n",
            "legyőzhetetlen kényszerek\n",
            "\n",
            "vezetnek vissza hozzád;\n",
            "\n",
            "folyam légy, s rajta én a hab,\n",
            "\n",
            "fogadd be tékozló fiad,\n",
            "\n",
            "komor, sötét mennyország.\n",
            "\n",
            "Éjféli fürdés\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "727tBebSqdeP",
        "colab_type": "code",
        "outputId": "43163466-cc0e-4d66-fcc9-74b42f45a14e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "clean_texts[10:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sem hegedű, se cimbalom,Beh szomorú lakodalom!Mintha orruk vére folyna -Dehogy lennék menyasszonya!',\n",
              " 'Jaj, ki vágyna menyasszonynak,Akit harangszóval hoznak!Gyászos ének búg előtte:Sírva mennek esketőre.',\n",
              " 'Nem menyasszony, - vőlegény volt:Hat legény hoz zöld koporsót,Apja, anyja sír előtte,Keserves a menyegzője.',\n",
              " 'Apját, anyját jól ismerem,Megmondanám, de nem merem,Mert a szívünk megdöbbenne...Egyikünké megrepedne.',\n",
              " 'Lyányok, lyányok, vegyetek felFehér ruhát s jőjetek elMa csak halott-látni,... holnapKivinni zöld koporsómat.',\n",
              " 'Ablak alattA pünkösdi rózsa,Kezd egy kicsitFesleni bimbója:Kékszemű lyány,Válogat belőle,KoszorúnakHolnap esküvőre.',\n",
              " 'ReménykedikEgy kis méh az ágon:Szép eladó,Jaj, ne bántsd virágom!Ezt az egyetMagamnak kerestem,Alig hasadtMikor eljegyeztem.',\n",
              " \"Felel a lyány:Te bohó kis állat!Lelsz te rózsátNem egyet, ha' százat,Holnap is nyit,Holnap is eljössz teCsak ne kívándAmi legszebb közte.\",\n",
              " 'Mond a kis méh:Szőke szép hajadon,Neked IstenHű szeretőt adjon!Nem sok amitKívánok tetőled:Ne szakaszd leAz én szeretőmet.',\n",
              " \"Felel a lyány:Dehogynem szakasztom!Dehogy leszekE ne'kűl menyasszony!KoszorúmbanEzt fonom előre,Ugy vigyenekHolnap esküvőre.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "-m1fCHxGySxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Tokenizer (which uses SentencePiece), Corpus ( data handler)"
      ]
    },
    {
      "metadata": {
        "id": "wGJipK5GmgsT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    \n",
        "    ID_UNK = 0\n",
        "    ID_SOS = 1\n",
        "    ID_EOS = 2\n",
        "    \n",
        "    def __init__(self, model_path:str):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.Load(str(model_path))\n",
        "        self.id_unk = self.sp.unk_id()\n",
        "        self.tk_unk = self.sp.IdToPiece(self.id_unk)\n",
        "        self.tk_sos = self.sp.IdToPiece(self.ID_SOS)\n",
        "        self.tk_eos = self.sp.IdToPiece(self.ID_EOS)\n",
        "        self.vocab_size = len(self.sp)\n",
        "        logger.info(f\"Initialized SentPieceProcessor from {model_path}\")\n",
        "    \n",
        "    def numericalize(self, tokens: List[str]) -> List[List[int]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        ids =  [self.sp.EncodeAsIds(s) for s in tokens]\n",
        "        if len(ids) == 1: ids=ids[0]\n",
        "        return ids\n",
        "\n",
        "    def piecify(self, tokens: List[str]) -> List[List[str]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        return [self.sp.EncodeAsPieces(s) for s in tokens]\n",
        "    \n",
        "    def textify(self, ids: List[int]) -> str:\n",
        "        if isinstance(ids, list) and isinstance(ids[0], np.generic): \n",
        "            ids = [int(x) for x in ids]\n",
        "        if not isinstance(ids, list) and not isinstance(ids[0], int):\n",
        "            raise TypeError(\"Argument `ids` has to be a list of integers.\")            \n",
        "        return self.sp.DecodeIds(ids)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_file(cls, input_file:str, output_path:str='default', vocab_size:int=16000, \n",
        "                  char_cov:float=1.0, model_type:str='unigram'):\n",
        "        \n",
        "        assert model_type in ['unigram', 'bpe', 'char', 'word']\n",
        "        assert 0 < char_cov <= 1\n",
        "        input_file = str(input_file)\n",
        "        output_file =  os.path.splitext(str(output_path))[0]\n",
        "        ext = '.model'\n",
        "        train_cmd = f\"--input={input_file} --model_prefix={output_file}\"\\\n",
        "                    f\" --vocab_size={vocab_size} --character_coverage={char_cov} --model_type={model_type}\"\n",
        "\n",
        "        logger.info(f\"Train command: {train_cmd}\")\n",
        "        logger.info(f\"Started training SentencePiece model...\")\n",
        "        ret = spm.SentencePieceTrainer.Train(train_cmd)\n",
        "        logger.info(f\"Exit code: {int(ret)}\")\n",
        "        return cls(output_file+ext)\n",
        "      \n",
        "def batchify(data, bsz):\n",
        "    # work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "class Corpus(object):\n",
        "\n",
        "  def __init__(self, file_path, tokenizer, bs=20):\n",
        "       \n",
        "        self.processor = tokenizer\n",
        "        self.id_sos, self.id_eos = self.processor.ID_SOS, self.processor.ID_EOS\n",
        "        self.bs = bs\n",
        "        self.data = self.tokenize(file_path)        \n",
        "        self.vocab_size = self.processor.vocab_size\n",
        "        \n",
        "  def tokenize(self, path):\n",
        "      \"\"\"Tokenizes a text file.\"\"\"\n",
        "      ids = []\n",
        "      with open(path, 'r') as f:\n",
        "          for line in f:\n",
        "              numericalized = self.processor.numericalize(line)\n",
        "              ids.extend([self.id_sos] + numericalized + [self.id_eos])\n",
        "\n",
        "      return batchify(torch.LongTensor(ids), self.bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTn7LTQfkJlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define RNN model architecture,  training loop and helpers"
      ]
    },
    {
      "metadata": {
        "id": "bi951Z9uzzCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, emsize, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, emsize)\n",
        "        assert (rnn_type in ['LSTM', 'GRU']), \"Arg `rnn_type` has to be one of {GRU, LSTM}.\"\n",
        "        self.rnn = getattr(nn, rnn_type)(emsize, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != emsize:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "       \n",
        "      \n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "  if isinstance(h, torch.Tensor):\n",
        "    return h.detach()\n",
        "  else:\n",
        "    return tuple(repackage_hidden(v) for v in h)\n",
        "      \n",
        "def get_num_params(model):\n",
        "  return sum([p.nelement() for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "def get_batch(source, i, bptt):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "  \n",
        "def train_epoch(train_data, model, vocab_size, bs=16, bptt=20, clip=.25):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    hidden = model.init_hidden(bs)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        total_loss += len(data) * loss.item()\n",
        "        \n",
        "    return total_loss / (len(train_data) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w62PRkgBJS7W",
        "colab_type": "code",
        "outputId": "f336b24a-bbba-4a33-f107-9c2b1e0ebd00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "# INIT AND TRAIN TOKENIZER FROM CLEAN TEXTS\n",
        "bs = 64\n",
        "\n",
        "model_type = 'bpe' # can be \"bpe\"/\"unigram\" to support wordpieces\n",
        "\n",
        "vocab_size = 24000 # how many wordpieces to consider\n",
        "\n",
        "tokenizer = Tokenizer.from_file(data_dir/'ady_clean.txt', \n",
        "                                output_path=data_dir/'tokenizer', char_cov=1.0,\n",
        "                                model_type=model_type, vocab_size=vocab_size) \n",
        "\n",
        "# INIT CORPUS FROM CLEAN TEXTS AND TOKENIZER WITH BATCH_SIZE `BS`\n",
        "\n",
        "corpus = Corpus(data_dir/'ady_clean.txt', tokenizer=tokenizer, bs=bs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-05 13:31:58,820 : INFO : Train command: --input=/content/data/ady-wp/ady_clean.txt --model_prefix=/content/data/ady-wp/tokenizer --vocab_size=24000 --character_coverage=1.0 --model_type=bpe\n",
            "2019-04-05 13:31:58,821 : INFO : Started training SentencePiece model...\n",
            "2019-04-05 13:32:06,362 : INFO : Exit code: 1\n",
            "2019-04-05 13:32:06,382 : INFO : Initialized SentPieceProcessor from /content/data/ady-wp/tokenizer.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3BDcrSMzjNP",
        "colab_type": "code",
        "outputId": "61ae837c-e763-4956-8d50-7404b5163028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# LSTM PARAMETERS\n",
        "model_type='GRU'\n",
        "\n",
        "emsize = 400\n",
        "nhid = 400\n",
        "\n",
        "nlayers = 1\n",
        "\n",
        "dropout = 0.15\n",
        "clip = 2.5\n",
        "\n",
        "tied = True\n",
        "\n",
        "bptt = 80\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "# INIT LSTM MODEL, LOSS FUNCTION AND OPTIMIZER\n",
        "\n",
        "model = RNNModel(model_type, corpus.vocab_size, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "print(f\"No. of parameters: {get_num_params(model)}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No. of parameters: 10586400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FXRCWjOZaBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's train the model"
      ]
    },
    {
      "metadata": {
        "id": "H3zNzPST0eYI",
        "colab_type": "code",
        "outputId": "a3c8aae9-af65-46b8-b140-c83871bd69c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2567
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 150\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "  \n",
        "  for epoch in range(1, epochs+1):\n",
        "\n",
        "    loss = train_epoch(corpus.data, model, corpus.vocab_size, \n",
        "                       clip=clip, bs=bs, bptt=bptt)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"epoch {:3d} | loss {:5.2f} | perplexity {:8.2f}| elapsed {:5.2f}s \".format(epoch, loss, math.exp(loss), elapsed))\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print('-' * 89)\n",
        "  print('Exiting from training early')\n",
        "\n",
        "finally:\n",
        "  params = {\"model_type\": model_type,\n",
        "            \"ntoken\": corpus.vocab_size,\n",
        "            \"emsize\": emsize,\n",
        "            \"nhid\": nhid,\n",
        "            \"nlayers\": nlayers,\n",
        "            \"dropout\": dropout,\n",
        "            \"tied\": tied}\n",
        " \n",
        "  with open(data_dir/'model_state.pth', 'wb') as f:\n",
        "    torch.save({\"state_dict\": model.state_dict(),\n",
        "                \"params\": params}, f)\n",
        "zipfile = zip_dir(data_dir, 'ady-wp')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | loss  6.94 | perplexity  1030.12| elapsed 11.73s \n",
            "epoch   2 | loss  5.75 | perplexity   315.40| elapsed 23.42s \n",
            "epoch   3 | loss  5.51 | perplexity   248.09| elapsed 35.04s \n",
            "epoch   4 | loss  5.35 | perplexity   210.96| elapsed 46.70s \n",
            "epoch   5 | loss  5.22 | perplexity   184.24| elapsed 58.38s \n",
            "epoch   6 | loss  5.09 | perplexity   163.02| elapsed 70.00s \n",
            "epoch   7 | loss  4.98 | perplexity   145.31| elapsed 81.69s \n",
            "epoch   8 | loss  4.86 | perplexity   129.55| elapsed 93.32s \n",
            "epoch   9 | loss  4.75 | perplexity   116.00| elapsed 104.98s \n",
            "epoch  10 | loss  4.64 | perplexity   104.06| elapsed 116.60s \n",
            "epoch  11 | loss  4.53 | perplexity    93.12| elapsed 128.26s \n",
            "epoch  12 | loss  4.43 | perplexity    83.71| elapsed 139.93s \n",
            "epoch  13 | loss  4.32 | perplexity    75.45| elapsed 151.56s \n",
            "epoch  14 | loss  4.22 | perplexity    67.78| elapsed 163.22s \n",
            "epoch  15 | loss  4.11 | perplexity    60.96| elapsed 174.86s \n",
            "epoch  16 | loss  4.00 | perplexity    54.76| elapsed 186.49s \n",
            "epoch  17 | loss  3.89 | perplexity    48.69| elapsed 198.18s \n",
            "epoch  18 | loss  3.77 | perplexity    43.46| elapsed 209.80s \n",
            "epoch  19 | loss  3.67 | perplexity    39.13| elapsed 221.47s \n",
            "epoch  20 | loss  3.56 | perplexity    35.04| elapsed 233.09s \n",
            "epoch  21 | loss  3.44 | perplexity    31.20| elapsed 244.74s \n",
            "epoch  22 | loss  3.33 | perplexity    27.99| elapsed 256.36s \n",
            "epoch  23 | loss  3.23 | perplexity    25.18| elapsed 268.05s \n",
            "epoch  24 | loss  3.12 | perplexity    22.64| elapsed 279.67s \n",
            "epoch  25 | loss  3.02 | perplexity    20.55| elapsed 291.31s \n",
            "epoch  26 | loss  2.92 | perplexity    18.63| elapsed 302.93s \n",
            "epoch  27 | loss  2.83 | perplexity    17.02| elapsed 314.55s \n",
            "epoch  28 | loss  2.75 | perplexity    15.59| elapsed 326.15s \n",
            "epoch  29 | loss  2.67 | perplexity    14.38| elapsed 337.79s \n",
            "epoch  30 | loss  2.59 | perplexity    13.32| elapsed 349.38s \n",
            "epoch  31 | loss  2.51 | perplexity    12.31| elapsed 361.05s \n",
            "epoch  32 | loss  2.43 | perplexity    11.39| elapsed 372.66s \n",
            "epoch  33 | loss  2.36 | perplexity    10.64| elapsed 384.32s \n",
            "epoch  34 | loss  2.30 | perplexity     9.96| elapsed 395.92s \n",
            "epoch  35 | loss  2.23 | perplexity     9.32| elapsed 407.57s \n",
            "epoch  36 | loss  2.16 | perplexity     8.69| elapsed 419.16s \n",
            "epoch  38 | loss  2.03 | perplexity     7.62| elapsed 442.37s \n",
            "epoch  39 | loss  1.97 | perplexity     7.14| elapsed 453.98s \n",
            "epoch  40 | loss  1.91 | perplexity     6.78| elapsed 465.56s \n",
            "epoch  41 | loss  1.86 | perplexity     6.42| elapsed 477.17s \n",
            "epoch  42 | loss  1.81 | perplexity     6.10| elapsed 488.76s \n",
            "epoch  43 | loss  1.76 | perplexity     5.83| elapsed 500.41s \n",
            "epoch  44 | loss  1.72 | perplexity     5.59| elapsed 512.01s \n",
            "epoch  45 | loss  1.68 | perplexity     5.37| elapsed 523.66s \n",
            "epoch  46 | loss  1.65 | perplexity     5.18| elapsed 535.23s \n",
            "epoch  47 | loss  1.61 | perplexity     5.01| elapsed 546.83s \n",
            "epoch  48 | loss  1.58 | perplexity     4.84| elapsed 558.42s \n",
            "epoch  49 | loss  1.55 | perplexity     4.70| elapsed 570.04s \n",
            "epoch  50 | loss  1.52 | perplexity     4.57| elapsed 581.63s \n",
            "epoch  51 | loss  1.49 | perplexity     4.43| elapsed 593.24s \n",
            "epoch  52 | loss  1.45 | perplexity     4.28| elapsed 604.83s \n",
            "epoch  53 | loss  1.42 | perplexity     4.15| elapsed 616.42s \n",
            "epoch  54 | loss  1.39 | perplexity     4.01| elapsed 627.99s \n",
            "epoch  55 | loss  1.36 | perplexity     3.88| elapsed 639.61s \n",
            "epoch  56 | loss  1.32 | perplexity     3.76| elapsed 651.17s \n",
            "epoch  57 | loss  1.29 | perplexity     3.64| elapsed 662.76s \n",
            "epoch  58 | loss  1.26 | perplexity     3.52| elapsed 674.34s \n",
            "epoch  59 | loss  1.22 | perplexity     3.40| elapsed 685.93s \n",
            "epoch  60 | loss  1.20 | perplexity     3.31| elapsed 697.52s \n",
            "epoch  61 | loss  1.17 | perplexity     3.22| elapsed 709.12s \n",
            "epoch  62 | loss  1.14 | perplexity     3.13| elapsed 720.69s \n",
            "epoch  63 | loss  1.12 | perplexity     3.05| elapsed 732.30s \n",
            "epoch  64 | loss  1.09 | perplexity     2.97| elapsed 743.88s \n",
            "epoch  65 | loss  1.07 | perplexity     2.91| elapsed 755.47s \n",
            "epoch  66 | loss  1.04 | perplexity     2.84| elapsed 767.06s \n",
            "epoch  67 | loss  1.02 | perplexity     2.78| elapsed 778.65s \n",
            "epoch  68 | loss  1.00 | perplexity     2.73| elapsed 790.19s \n",
            "epoch  69 | loss  0.98 | perplexity     2.66| elapsed 801.83s \n",
            "epoch  70 | loss  0.96 | perplexity     2.62| elapsed 813.43s \n",
            "epoch  71 | loss  0.94 | perplexity     2.57| elapsed 825.04s \n",
            "epoch  72 | loss  0.92 | perplexity     2.52| elapsed 836.63s \n",
            "epoch  73 | loss  0.91 | perplexity     2.48| elapsed 848.23s \n",
            "epoch  74 | loss  0.90 | perplexity     2.45| elapsed 859.81s \n",
            "epoch  75 | loss  0.88 | perplexity     2.41| elapsed 871.41s \n",
            "epoch  76 | loss  0.87 | perplexity     2.38| elapsed 882.98s \n",
            "epoch  77 | loss  0.85 | perplexity     2.34| elapsed 894.57s \n",
            "epoch  78 | loss  0.84 | perplexity     2.31| elapsed 906.15s \n",
            "epoch  79 | loss  0.83 | perplexity     2.29| elapsed 917.75s \n",
            "epoch  80 | loss  0.81 | perplexity     2.26| elapsed 929.32s \n",
            "epoch  81 | loss  0.80 | perplexity     2.23| elapsed 940.91s \n",
            "epoch  82 | loss  0.79 | perplexity     2.21| elapsed 952.48s \n",
            "epoch  83 | loss  0.78 | perplexity     2.18| elapsed 964.06s \n",
            "epoch  84 | loss  0.77 | perplexity     2.16| elapsed 975.63s \n",
            "epoch  85 | loss  0.76 | perplexity     2.13| elapsed 987.23s \n",
            "epoch  86 | loss  0.75 | perplexity     2.11| elapsed 998.81s \n",
            "epoch  87 | loss  0.73 | perplexity     2.08| elapsed 1010.39s \n",
            "epoch  88 | loss  0.72 | perplexity     2.06| elapsed 1021.94s \n",
            "epoch  89 | loss  0.71 | perplexity     2.04| elapsed 1033.52s \n",
            "epoch  90 | loss  0.71 | perplexity     2.02| elapsed 1045.06s \n",
            "epoch  91 | loss  0.70 | perplexity     2.01| elapsed 1056.66s \n",
            "epoch  92 | loss  0.68 | perplexity     1.98| elapsed 1068.22s \n",
            "epoch  93 | loss  0.68 | perplexity     1.97| elapsed 1079.82s \n",
            "epoch  94 | loss  0.66 | perplexity     1.94| elapsed 1091.38s \n",
            "epoch  95 | loss  0.65 | perplexity     1.92| elapsed 1102.95s \n",
            "epoch  96 | loss  0.65 | perplexity     1.91| elapsed 1114.50s \n",
            "epoch  97 | loss  0.64 | perplexity     1.89| elapsed 1126.08s \n",
            "epoch  98 | loss  0.63 | perplexity     1.88| elapsed 1137.62s \n",
            "epoch  99 | loss  0.62 | perplexity     1.86| elapsed 1149.20s \n",
            "epoch 100 | loss  0.62 | perplexity     1.85| elapsed 1160.75s \n",
            "epoch 101 | loss  0.61 | perplexity     1.83| elapsed 1172.33s \n",
            "epoch 102 | loss  0.60 | perplexity     1.82| elapsed 1183.90s \n",
            "epoch 103 | loss  0.59 | perplexity     1.81| elapsed 1195.48s \n",
            "epoch 104 | loss  0.59 | perplexity     1.80| elapsed 1207.03s \n",
            "epoch 105 | loss  0.58 | perplexity     1.78| elapsed 1218.62s \n",
            "epoch 106 | loss  0.57 | perplexity     1.76| elapsed 1230.18s \n",
            "epoch 107 | loss  0.56 | perplexity     1.76| elapsed 1241.76s \n",
            "epoch 108 | loss  0.56 | perplexity     1.75| elapsed 1253.31s \n",
            "epoch 109 | loss  0.55 | perplexity     1.73| elapsed 1264.89s \n",
            "epoch 110 | loss  0.54 | perplexity     1.72| elapsed 1276.42s \n",
            "epoch 111 | loss  0.53 | perplexity     1.71| elapsed 1288.02s \n",
            "epoch 112 | loss  0.53 | perplexity     1.70| elapsed 1299.59s \n",
            "epoch 113 | loss  0.53 | perplexity     1.69| elapsed 1311.15s \n",
            "epoch 114 | loss  0.52 | perplexity     1.68| elapsed 1322.74s \n",
            "epoch 115 | loss  0.51 | perplexity     1.67| elapsed 1334.33s \n",
            "epoch 116 | loss  0.51 | perplexity     1.66| elapsed 1345.89s \n",
            "epoch 117 | loss  0.50 | perplexity     1.65| elapsed 1357.48s \n",
            "epoch 118 | loss  0.50 | perplexity     1.64| elapsed 1369.04s \n",
            "epoch 119 | loss  0.49 | perplexity     1.64| elapsed 1380.62s \n",
            "epoch 120 | loss  0.49 | perplexity     1.63| elapsed 1392.17s \n",
            "epoch 121 | loss  0.48 | perplexity     1.62| elapsed 1403.76s \n",
            "epoch 122 | loss  0.48 | perplexity     1.61| elapsed 1415.31s \n",
            "epoch 123 | loss  0.47 | perplexity     1.60| elapsed 1426.88s \n",
            "epoch 124 | loss  0.46 | perplexity     1.59| elapsed 1438.42s \n",
            "epoch 125 | loss  0.46 | perplexity     1.59| elapsed 1449.98s \n",
            "epoch 126 | loss  0.45 | perplexity     1.58| elapsed 1461.54s \n",
            "epoch 127 | loss  0.45 | perplexity     1.57| elapsed 1473.13s \n",
            "epoch 128 | loss  0.45 | perplexity     1.56| elapsed 1484.70s \n",
            "epoch 129 | loss  0.45 | perplexity     1.56| elapsed 1496.29s \n",
            "epoch 130 | loss  0.44 | perplexity     1.55| elapsed 1507.82s \n",
            "epoch 131 | loss  0.43 | perplexity     1.54| elapsed 1519.39s \n",
            "epoch 132 | loss  0.43 | perplexity     1.53| elapsed 1530.95s \n",
            "epoch 133 | loss  0.42 | perplexity     1.53| elapsed 1542.53s \n",
            "epoch 134 | loss  0.42 | perplexity     1.52| elapsed 1554.05s \n",
            "epoch 135 | loss  0.42 | perplexity     1.51| elapsed 1565.64s \n",
            "epoch 136 | loss  0.41 | perplexity     1.51| elapsed 1577.23s \n",
            "epoch 137 | loss  0.41 | perplexity     1.50| elapsed 1588.80s \n",
            "epoch 138 | loss  0.40 | perplexity     1.49| elapsed 1600.34s \n",
            "epoch 139 | loss  0.40 | perplexity     1.49| elapsed 1611.95s \n",
            "epoch 140 | loss  0.40 | perplexity     1.49| elapsed 1623.50s \n",
            "epoch 141 | loss  0.39 | perplexity     1.48| elapsed 1635.07s \n",
            "epoch 142 | loss  0.39 | perplexity     1.48| elapsed 1646.60s \n",
            "epoch 143 | loss  0.39 | perplexity     1.47| elapsed 1658.14s \n",
            "epoch 144 | loss  0.38 | perplexity     1.46| elapsed 1669.69s \n",
            "epoch 145 | loss  0.38 | perplexity     1.46| elapsed 1681.28s \n",
            "epoch 146 | loss  0.37 | perplexity     1.45| elapsed 1692.81s \n",
            "epoch 147 | loss  0.37 | perplexity     1.45| elapsed 1704.41s \n",
            "epoch 148 | loss  0.37 | perplexity     1.44| elapsed 1715.99s \n",
            "epoch 149 | loss  0.37 | perplexity     1.44| elapsed 1727.53s \n",
            "epoch 150 | loss  0.36 | perplexity     1.44| elapsed 1739.09s \n",
            "Zipped files at /content/data/ady-wp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oueW91h7bjak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate verses! "
      ]
    },
    {
      "metadata": {
        "id": "vtS63EkMOIbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "import torch\n",
        "\n",
        "def is_unbalanced(s):\n",
        "  if s.count('\"') % 2 != 0 or s.count('(') != s.count(')'):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def sample_punkt():\n",
        "  return sample(['.', '?', '!'], 1)[0]\n",
        "\n",
        "def parse_last_line(s):\n",
        "  l = list(s)\n",
        "  \n",
        "  if l[-1] == ',': \n",
        "    l[-1] = sample_punkt()\n",
        "  \n",
        "  if l[-1] not in list('.?!'): \n",
        "    l.append(sample_punkt())  \n",
        "  return \"\".join(l)\n",
        "  \n",
        "def generate_line(model, hidden=None, temp=1.0, \n",
        "               sos_id=1, eos_id=2, unk_id=0, max_len=None):\n",
        "  \"\"\"Generate line from `model` with `hidden` state at `temp`.\"\"\"\n",
        "  ids = []\n",
        "  \n",
        "  if hidden is None:\n",
        "    hidden = model.init_hidden(1)\n",
        "  \n",
        "  input = torch.tensor([sos_id], dtype=torch.long).reshape(1,1).to(device)\n",
        "  \n",
        "  id = 0\n",
        "  while id != eos_id and len(ids)<max_len :\n",
        "    output, hidden = model(input, hidden)\n",
        "    probs = output.squeeze().div(temp).exp().cpu() \n",
        "    id = torch.multinomial(probs, num_samples=1).item() \n",
        "    if id == sos_id or id == unk_id: continue\n",
        "    input.fill_(id)\n",
        "    ids += [id]\n",
        "  \n",
        "  return ids, hidden\n",
        "\n",
        "def generate(model, tokenizer, num_lines=8, min_len=8, max_len=15,\n",
        "             unk_id=0, sos_id=1, eos_id=2, temp=0.6):\n",
        "  \"\"\" \n",
        "  Generate a verse consisting of `num_lines` lines of max. length `max_tokens`.\n",
        "  Since the hidden state is passed onto the next line, \n",
        "  observing some cross-line consistency would be expected, or less\n",
        "  optimistically, at least grammatically correct sentences.\n",
        "  NOTE: line length can be tuned by changing max_tokens (i.e. subword pieces).\n",
        "  \n",
        "  Args:\n",
        "    model: Trained PyTorch language model\n",
        "    tokenizer: SentencePiece tokenizer\n",
        "    temp: Temperature parameter; lower: more conservative, higher: more diverse\n",
        "    num_lines: No. of lines to generate.\n",
        "    max_len: Max no. of tokens per line (not words!)\n",
        "    sos_id: Start of sequence id in vocabulary\n",
        "    eos_id: End of sequence id in vocabulary\n",
        "  \n",
        "  Returns: list of strings\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  \n",
        "  lines = []\n",
        "  line_cnt = 0\n",
        "  hidden = model.init_hidden(1)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    while line_cnt != num_lines:\n",
        "      try:\n",
        "\n",
        "        ids, hidden = generate_line(model, hidden=hidden, temp=temp, max_len=max_len,\n",
        "                                    sos_id=sos_id, eos_id=eos_id, unk_id=unk_id)\n",
        "        \n",
        "        if len(ids) <= min_len: raise Exception\n",
        "        line = tokenizer.textify(ids).strip()\n",
        "        \n",
        "        if line.startswith(tuple(\"-?!.,()\")): raise Exception\n",
        "        if is_unbalanced(line): raise Exception\n",
        "        \n",
        "        lines += [line]\n",
        "        line_cnt +=1\n",
        "        \n",
        "      except Exception as e:\n",
        "        pass\n",
        "    \n",
        "  last_line = lines.pop()\n",
        "  l = parse_last_line(last_line)\n",
        "  lines.append(l)\n",
        "  \n",
        "  return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73w_4CMz-Mrv",
        "colab_type": "code",
        "outputId": "01a64066-06a2-4322-b5be-0a2a492596d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.5, num_lines=8, min_len=8, max_len=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['S én tisztulóan lesem, lesem,',\n",
              " 'végre is e dühös vakok, nóta!...',\n",
              " 'S én vagyok, hogy most már nincs itt.',\n",
              " 'Fény-emberem, végtelenbe, kálvinisták,',\n",
              " 'tyúk-virág, én régi, bús Magyarország.',\n",
              " 'elő egy szent hír úgy prédák,',\n",
              " 'és szép, kit talán már nem is szitkozódik az Őszt is.',\n",
              " 'És te ne búsulj, de dalolj.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "9mCMMjXJQOQ-",
        "colab_type": "code",
        "outputId": "5b9c069e-011c-4f54-a4b6-2aee1f3f7ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.6, num_lines=8, max_len=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ázó, a tenger álmos, szegény.',\n",
              " 'És én, hogy vagyok és hogy én',\n",
              " 'Az Ősz csókja sa-ott,',\n",
              " 'S én tisztulóan lesem, lesem,',\n",
              " 'Küldött az én! S a régi, lázadt jobbágyok sehol,',\n",
              " 'okban keresik az emberek a halált, de nem találják meg azt és kívánnak meghalni',\n",
              " 'jek?... hogy nem is vár fészek,',\n",
              " 'S ez a Halál, ha elragadja.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "_yNUM_HPSpvw",
        "colab_type": "code",
        "outputId": "e714285f-dd1a-4675-d459-817adfb09425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.7, num_lines=10, max_len=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['S mint az Úr: a Láz.',\n",
              " 'S egy-két ember, ki több őrületben,',\n",
              " 'legyen: a Vér, a tűrés,',\n",
              " 'S az én is, de ki vagyok.',\n",
              " 'S amit én, mint sors-hont:',\n",
              " 'va, kedves csók-vajtott.',\n",
              " 'Sanak egy fehér-képen szakad.',\n",
              " 'És akkor talán őt is, talán hogy',\n",
              " 'S szegény virág-e még mint a Csönd.',\n",
              " 'Annak okáért olyanná lésznek, mint a reggeli köd.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    }
  ]
}