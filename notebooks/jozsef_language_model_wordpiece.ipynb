{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jozsef-language-model-wordpiece.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben0it8/poetry-language-model/blob/master/jozsef_language_model_wordpiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jpXivGD0YpHn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Check GPU memory availability"
      ]
    },
    {
      "metadata": {
        "id": "NVEc6D3Tlsnl",
        "colab_type": "code",
        "outputId": "207a8ec6-735c-4ba0-ea1d-99fad42f87b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 249.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Syp72_eY06e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Install extra requirements"
      ]
    },
    {
      "metadata": {
        "id": "EAGNkWy_D5er",
        "colab_type": "code",
        "outputId": "577c6a0f-848e-4760-fef6-4d98fd17ea18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U ftfy\n",
        "!pip install -U sentencepiece"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: ftfy in /usr/local/lib/python3.6/dist-packages (5.5.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Requirement already up-to-date: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.81)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYlrr4nDEk5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'; # adapt plots for retina displays\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid');\n",
        "sns.set_context(context='notebook');\n",
        "from typing import List\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import numpy as np \n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import ftfy \n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import dill\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.nn import functional as F\n",
        "from random import sample\n",
        "\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "data_dir = Path(\"data/jozsef-wp\").resolve()\n",
        "data_dir.mkdir(exist_ok=True, parents=True)\n",
        "url = \"https://raw.githubusercontent.com/ben0it8/ady/master/data/jozsef.txt\" \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIaijzWl2G-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_paths(path):\n",
        "  path = Path(path)\n",
        "  ret = []\n",
        "  for x in path.glob('*'):\n",
        "    if x.is_file() and not str(x).endswith('.zip'):\n",
        "      ret.append(str(x))\n",
        "  return ret\n",
        "\n",
        "def zip_dir(path, name):\n",
        "  if not name.endswith('.zip'): name += '.zip'\n",
        "  file_paths = get_file_paths(path)\n",
        "  \n",
        "  with ZipFile(path/name, 'w') as zip:\n",
        "    for file in file_paths:\n",
        "      zip.write(file, os.path.basename(file))\n",
        "  print(f\"Zipped files at {path}\")\n",
        "  return (path/name).resolve()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIxEBl5Irzky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Read data"
      ]
    },
    {
      "metadata": {
        "id": "-HobfQaar-ID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean data & write to disk"
      ]
    },
    {
      "metadata": {
        "id": "LidRmCTeFDP-",
        "colab_type": "code",
        "outputId": "931c2230-8c25-4a6c-87a4-088e615a7e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def is_title(t):\n",
        "  t = re.sub('[\\d\\s\\s\\-!?,]', '', t)\n",
        "  return t.isupper()\n",
        "\n",
        "def is_dateline(t):\n",
        "  if any(m in t for m in ['jan.', 'febr.', 'márc.', 'ápr.', 'jún.', 'júl.', 'aug.', 'szept.',\n",
        "      'okt.', 'nov.', 'dec.']):\n",
        "    return True\n",
        "  else: False\n",
        "    \n",
        "def fix_text(t:str):\n",
        "  t = ftfy.fix_text(t, normalization='NFC')\n",
        "  t = t.replace('\\n', '') # remove newlines\n",
        "  t = re.sub(r'[»«]', '', t) # remove special parenthesis\n",
        "  t = re.sub(r'[0-9]','', t)\n",
        "  t = re.sub(\"\\s\\s+\", \" \", t) # skip whitespaces\n",
        "  t = t.strip()\n",
        "  return t\n",
        "    \n",
        "def fix_texts(texts:list):\n",
        "  out = []\n",
        "  for i, line in enumerate(texts):\n",
        "    if \"\\u2424\" in line:\n",
        "      line = line.split(\"\\u2424\")\n",
        "    elif '\\u2028' in line:\n",
        "      line = line.split('\\u2028')\n",
        "    elif '\\u000A' in line:\n",
        "      line = line.split('\\u000A')\n",
        "    else:\n",
        "      line = [line] \n",
        "    for t in line:\n",
        "      t = fix_text(t)\n",
        "      if (t is None or len(t.replace(' ', ''))<=3 or is_title(t) or\n",
        "          t.startswith(('.', ',', '?', '!', '-', ';')) or len(t) > 100):\n",
        "        continue\n",
        "      else:\n",
        "        out += [t]   \n",
        "  return out\n",
        "\n",
        "\n",
        "response =  requests.get(url)\n",
        "texts = [line.decode('latin-1') for line in response.iter_lines()]\n",
        "print(f\"No. of lines: {len(texts)}\")\n",
        "clean_texts = fix_texts(texts)\n",
        "(data_dir/'text_clean.txt').open(mode='wt').writelines(f\"{line}\\n\" for line in clean_texts)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of lines: 18035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-m1fCHxGySxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Tokenizer (which uses SentencePiece), Corpus ( data handler)"
      ]
    },
    {
      "metadata": {
        "id": "wGJipK5GmgsT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    \n",
        "    ID_UNK = 0\n",
        "    ID_SOS = 1\n",
        "    ID_EOS = 2\n",
        "    \n",
        "    def __init__(self, model_path:str):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.Load(str(model_path))\n",
        "        self.id_unk = self.sp.unk_id()\n",
        "        self.tk_unk = self.sp.IdToPiece(self.id_unk)\n",
        "        self.tk_sos = self.sp.IdToPiece(self.ID_SOS)\n",
        "        self.tk_eos = self.sp.IdToPiece(self.ID_EOS)\n",
        "        self.vocab_size = len(self.sp)\n",
        "        logger.info(f\"Initialized SentPieceProcessor from {model_path}\")\n",
        "    \n",
        "    def numericalize(self, tokens: List[str]) -> List[List[int]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        ids =  [self.sp.EncodeAsIds(s) for s in tokens]\n",
        "        if len(ids) == 1: ids=ids[0]\n",
        "        return ids\n",
        "\n",
        "    def piecify(self, tokens: List[str]) -> List[List[str]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        return [self.sp.EncodeAsPieces(s) for s in tokens]\n",
        "    \n",
        "    def textify(self, ids: List[int]) -> str:\n",
        "        if isinstance(ids, list) and isinstance(ids[0], np.generic): \n",
        "            ids = [int(x) for x in ids]\n",
        "        if not isinstance(ids, list) and not isinstance(ids[0], int):\n",
        "            raise TypeError(\"Argument `ids` has to be a list of integers.\")            \n",
        "        return self.sp.DecodeIds(ids)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_file(cls, input_file:str, output_path:str='default', vocab_size:int=16000, \n",
        "                  char_cov:float=1.0, model_type:str='unigram'):\n",
        "        \n",
        "        assert model_type in ['unigram', 'bpe', 'char', 'word']\n",
        "        assert 0 < char_cov <= 1\n",
        "        input_file = str(input_file)\n",
        "        output_file =  os.path.splitext(str(output_path))[0]\n",
        "        ext = '.model'\n",
        "        train_cmd = f\"--input={input_file} --model_prefix={output_file}\"\\\n",
        "                    f\" --vocab_size={vocab_size} --character_coverage={char_cov} --model_type={model_type}\"\n",
        "\n",
        "        logger.info(f\"Train command: {train_cmd}\")\n",
        "        logger.info(f\"Started training SentencePiece model...\")\n",
        "        ret = spm.SentencePieceTrainer.Train(train_cmd)\n",
        "        logger.info(f\"Exit code: {int(ret)}\")\n",
        "        return cls(output_file+ext)\n",
        "      \n",
        "def batchify(data, bsz):\n",
        "    # work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "class Corpus(object):\n",
        "\n",
        "  def __init__(self, file_path, tokenizer, bs=20):\n",
        "       \n",
        "        self.processor = tokenizer\n",
        "        self.id_sos, self.id_eos = self.processor.ID_SOS, self.processor.ID_EOS\n",
        "        self.bs = bs\n",
        "        self.data = self.tokenize(file_path)        \n",
        "        self.vocab_size = self.processor.vocab_size\n",
        "        \n",
        "  def tokenize(self, path):\n",
        "      \"\"\"Tokenizes a text file.\"\"\"\n",
        "      ids = []\n",
        "      with open(path, 'r') as f:\n",
        "          for line in f:\n",
        "              numericalized = self.processor.numericalize(line)\n",
        "              ids.extend([self.id_sos] + numericalized + [self.id_eos])\n",
        "\n",
        "      return batchify(torch.LongTensor(ids), self.bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTn7LTQfkJlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define RNN model architecture,  training loop and helpers"
      ]
    },
    {
      "metadata": {
        "id": "bi951Z9uzzCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, emsize, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, emsize)\n",
        "        assert (rnn_type in ['LSTM', 'GRU']), \"Arg `rnn_type` has to be one of {GRU, LSTM}.\"\n",
        "        self.rnn = getattr(nn, rnn_type)(emsize, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != emsize:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "       \n",
        "      \n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "  if isinstance(h, torch.Tensor):\n",
        "    return h.detach()\n",
        "  else:\n",
        "    return tuple(repackage_hidden(v) for v in h)\n",
        "      \n",
        "def get_num_params(model):\n",
        "  return sum([p.nelement() for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "def get_batch(source, i, bptt):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "  \n",
        "def train_epoch(train_data, model, vocab_size, bs=16, bptt=20, clip=.25):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    hidden = model.init_hidden(bs)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        total_loss += len(data) * loss.item()\n",
        "        \n",
        "    return total_loss / (len(train_data) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w62PRkgBJS7W",
        "colab_type": "code",
        "outputId": "e4ca0e87-5571-4b2e-b584-27a7c2aa775d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "# INIT AND TRAIN TOKENIZER FROM CLEAN TEXTS\n",
        "bs = 32\n",
        "\n",
        "model_type = 'unigram' # can be \"bpe\"/\"unigram\" to support wordpieces\n",
        "\n",
        "vocab_size = 16000 # how many wordpieces to consider\n",
        "\n",
        "tokenizer = Tokenizer.from_file(data_dir/'text_clean.txt', \n",
        "                                output_path=data_dir/'tokenizer', char_cov=1.0,\n",
        "                                model_type=model_type, vocab_size=vocab_size) \n",
        "\n",
        "# INIT CORPUS FROM CLEAN TEXTS AND TOKENIZER WITH BATCH_SIZE `BS`\n",
        "\n",
        "corpus = Corpus(data_dir/'text_clean.txt', tokenizer=tokenizer, bs=bs)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-10 15:46:56,868 : INFO : Train command: --input=/content/data/jozsef-wp/text_clean.txt --model_prefix=/content/data/jozsef-wp/tokenizer --vocab_size=16000 --character_coverage=1.0 --model_type=unigram\n",
            "2019-04-10 15:46:56,871 : INFO : Started training SentencePiece model...\n",
            "2019-04-10 15:46:57,931 : INFO : Exit code: 1\n",
            "2019-04-10 15:46:57,962 : INFO : Initialized SentPieceProcessor from /content/data/jozsef-wp/tokenizer.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3BDcrSMzjNP",
        "colab_type": "code",
        "outputId": "2861c681-44e0-41a9-e06c-f58b8826d570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# LSTM PARAMETERS\n",
        "model_type='GRU'\n",
        "\n",
        "emsize = 300\n",
        "nhid = 300\n",
        "\n",
        "nlayers = 1\n",
        "\n",
        "dropout = 0.25\n",
        "clip = 2.5\n",
        "\n",
        "tied = True\n",
        "\n",
        "bptt = 80\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "# INIT LSTM MODEL, LOSS FUNCTION AND OPTIMIZER\n",
        "\n",
        "model = RNNModel(model_type, corpus.vocab_size, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "print(f\"No. of parameters: {get_num_params(model)}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of parameters: 5357800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FXRCWjOZaBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's train the model"
      ]
    },
    {
      "metadata": {
        "id": "H3zNzPST0eYI",
        "colab_type": "code",
        "outputId": "a083aec6-1750-4845-b35c-f3e5c8f7a176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2584
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 150\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "  \n",
        "  for epoch in range(1, epochs+1):\n",
        "\n",
        "    loss = train_epoch(corpus.data, model, corpus.vocab_size, \n",
        "                       clip=clip, bs=bs, bptt=bptt)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"epoch {:3d} | loss {:5.2f} | perplexity {:8.2f}| elapsed {:5.2f}s \".format(epoch, loss, math.exp(loss), elapsed))\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print('-' * 89)\n",
        "  print('Exiting from training early')\n",
        "\n",
        "finally:\n",
        "  params = {\"model_type\": model_type,\n",
        "            \"ntoken\": corpus.vocab_size,\n",
        "            \"emsize\": emsize,\n",
        "            \"nhid\": nhid,\n",
        "            \"nlayers\": nlayers,\n",
        "            \"dropout\": dropout,\n",
        "            \"tied\": tied}\n",
        " \n",
        "  with open(data_dir/'model_state.pth', 'wb') as f:\n",
        "    torch.save({\"state_dict\": model.state_dict(),\n",
        "                \"params\": params}, f)\n",
        "    \n",
        "zipfile = zip_dir(data_dir, data_dir.name)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | loss  7.10 | perplexity  1212.40| elapsed  4.34s \n",
            "epoch   2 | loss  6.04 | perplexity   419.14| elapsed  8.51s \n",
            "epoch   3 | loss  5.84 | perplexity   344.08| elapsed 12.79s \n",
            "epoch   4 | loss  5.71 | perplexity   300.47| elapsed 16.97s \n",
            "epoch   5 | loss  5.58 | perplexity   264.88| elapsed 21.25s \n",
            "epoch   6 | loss  5.46 | perplexity   235.19| elapsed 25.46s \n",
            "epoch   7 | loss  5.34 | perplexity   209.37| elapsed 29.79s \n",
            "epoch   8 | loss  5.23 | perplexity   187.67| elapsed 34.00s \n",
            "epoch   9 | loss  5.13 | perplexity   168.70| elapsed 38.18s \n",
            "epoch  10 | loss  5.02 | perplexity   151.64| elapsed 42.45s \n",
            "epoch  11 | loss  4.91 | perplexity   135.89| elapsed 46.75s \n",
            "epoch  12 | loss  4.81 | perplexity   122.17| elapsed 50.95s \n",
            "epoch  13 | loss  4.70 | perplexity   109.71| elapsed 55.14s \n",
            "epoch  14 | loss  4.59 | perplexity    98.35| elapsed 59.44s \n",
            "epoch  15 | loss  4.48 | perplexity    88.21| elapsed 63.63s \n",
            "epoch  16 | loss  4.37 | perplexity    79.10| elapsed 67.92s \n",
            "epoch  17 | loss  4.27 | perplexity    71.26| elapsed 72.10s \n",
            "epoch  18 | loss  4.16 | perplexity    64.11| elapsed 76.50s \n",
            "epoch  19 | loss  4.06 | perplexity    57.82| elapsed 80.69s \n",
            "epoch  20 | loss  3.95 | perplexity    52.16| elapsed 84.89s \n",
            "epoch  21 | loss  3.85 | perplexity    46.84| elapsed 89.08s \n",
            "epoch  22 | loss  3.73 | perplexity    41.80| elapsed 93.28s \n",
            "epoch  23 | loss  3.62 | perplexity    37.47| elapsed 97.49s \n",
            "epoch  24 | loss  3.52 | perplexity    33.85| elapsed 101.70s \n",
            "epoch  25 | loss  3.42 | perplexity    30.71| elapsed 106.09s \n",
            "epoch  26 | loss  3.33 | perplexity    28.04| elapsed 110.31s \n",
            "epoch  27 | loss  3.24 | perplexity    25.65| elapsed 114.61s \n",
            "epoch  28 | loss  3.16 | perplexity    23.51| elapsed 118.82s \n",
            "epoch  29 | loss  3.07 | perplexity    21.62| elapsed 123.02s \n",
            "epoch  30 | loss  3.00 | perplexity    20.01| elapsed 127.31s \n",
            "epoch  31 | loss  2.92 | perplexity    18.55| elapsed 131.54s \n",
            "epoch  32 | loss  2.85 | perplexity    17.30| elapsed 135.85s \n",
            "epoch  33 | loss  2.78 | perplexity    16.15| elapsed 140.17s \n",
            "epoch  34 | loss  2.72 | perplexity    15.12| elapsed 145.05s \n",
            "epoch  35 | loss  2.66 | perplexity    14.23| elapsed 150.02s \n",
            "epoch  36 | loss  2.60 | perplexity    13.44| elapsed 154.92s \n",
            "epoch  37 | loss  2.55 | perplexity    12.77| elapsed 160.07s \n",
            "epoch  38 | loss  2.49 | perplexity    12.12| elapsed 165.17s \n",
            "epoch  39 | loss  2.44 | perplexity    11.51| elapsed 170.25s \n",
            "epoch  40 | loss  2.39 | perplexity    10.89| elapsed 174.44s \n",
            "epoch  41 | loss  2.34 | perplexity    10.42| elapsed 180.79s \n",
            "epoch  42 | loss  2.30 | perplexity    10.01| elapsed 192.42s \n",
            "epoch  43 | loss  2.26 | perplexity     9.62| elapsed 203.88s \n",
            "epoch  44 | loss  2.23 | perplexity     9.31| elapsed 215.45s \n",
            "epoch  45 | loss  2.19 | perplexity     8.90| elapsed 227.14s \n",
            "epoch  46 | loss  2.15 | perplexity     8.60| elapsed 238.57s \n",
            "epoch  47 | loss  2.11 | perplexity     8.24| elapsed 250.13s \n",
            "epoch  48 | loss  2.07 | perplexity     7.94| elapsed 261.77s \n",
            "epoch  49 | loss  2.04 | perplexity     7.68| elapsed 273.41s \n",
            "epoch  50 | loss  2.01 | perplexity     7.43| elapsed 285.08s \n",
            "epoch  51 | loss  1.97 | perplexity     7.15| elapsed 296.63s \n",
            "epoch  52 | loss  1.93 | perplexity     6.92| elapsed 308.19s \n",
            "epoch  53 | loss  1.91 | perplexity     6.72| elapsed 319.64s \n",
            "epoch  54 | loss  1.88 | perplexity     6.54| elapsed 331.59s \n",
            "epoch  55 | loss  1.85 | perplexity     6.37| elapsed 343.23s \n",
            "epoch  56 | loss  1.82 | perplexity     6.16| elapsed 354.74s \n",
            "epoch  57 | loss  1.79 | perplexity     6.01| elapsed 366.31s \n",
            "epoch  58 | loss  1.77 | perplexity     5.86| elapsed 377.81s \n",
            "epoch  59 | loss  1.75 | perplexity     5.73| elapsed 389.45s \n",
            "epoch  60 | loss  1.72 | perplexity     5.58| elapsed 401.03s \n",
            "epoch  61 | loss  1.69 | perplexity     5.44| elapsed 412.63s \n",
            "epoch  62 | loss  1.67 | perplexity     5.31| elapsed 424.23s \n",
            "epoch  63 | loss  1.65 | perplexity     5.19| elapsed 435.80s \n",
            "epoch  64 | loss  1.63 | perplexity     5.10| elapsed 447.45s \n",
            "epoch  65 | loss  1.61 | perplexity     4.99| elapsed 459.16s \n",
            "epoch  66 | loss  1.58 | perplexity     4.88| elapsed 470.78s \n",
            "epoch  67 | loss  1.57 | perplexity     4.80| elapsed 482.42s \n",
            "epoch  68 | loss  1.54 | perplexity     4.68| elapsed 494.04s \n",
            "epoch  69 | loss  1.53 | perplexity     4.60| elapsed 505.73s \n",
            "epoch  70 | loss  1.51 | perplexity     4.53| elapsed 517.51s \n",
            "epoch  71 | loss  1.49 | perplexity     4.43| elapsed 529.15s \n",
            "epoch  72 | loss  1.47 | perplexity     4.36| elapsed 540.71s \n",
            "epoch  73 | loss  1.46 | perplexity     4.30| elapsed 552.38s \n",
            "epoch  74 | loss  1.44 | perplexity     4.24| elapsed 564.05s \n",
            "epoch  75 | loss  1.43 | perplexity     4.17| elapsed 575.89s \n",
            "epoch  76 | loss  1.42 | perplexity     4.12| elapsed 587.74s \n",
            "epoch  77 | loss  1.40 | perplexity     4.05| elapsed 599.31s \n",
            "epoch  78 | loss  1.39 | perplexity     4.00| elapsed 611.00s \n",
            "epoch  79 | loss  1.37 | perplexity     3.95| elapsed 622.73s \n",
            "epoch  80 | loss  1.36 | perplexity     3.89| elapsed 634.39s \n",
            "epoch  81 | loss  1.34 | perplexity     3.82| elapsed 646.07s \n",
            "epoch  82 | loss  1.34 | perplexity     3.80| elapsed 657.75s \n",
            "epoch  83 | loss  1.32 | perplexity     3.74| elapsed 669.39s \n",
            "epoch  84 | loss  1.31 | perplexity     3.70| elapsed 681.03s \n",
            "epoch  85 | loss  1.30 | perplexity     3.66| elapsed 692.66s \n",
            "epoch  86 | loss  1.28 | perplexity     3.61| elapsed 704.30s \n",
            "epoch  87 | loss  1.28 | perplexity     3.59| elapsed 715.97s \n",
            "epoch  88 | loss  1.26 | perplexity     3.53| elapsed 727.69s \n",
            "epoch  89 | loss  1.25 | perplexity     3.51| elapsed 739.31s \n",
            "epoch  90 | loss  1.24 | perplexity     3.46| elapsed 751.00s \n",
            "epoch  91 | loss  1.23 | perplexity     3.42| elapsed 762.45s \n",
            "epoch  92 | loss  1.22 | perplexity     3.39| elapsed 774.11s \n",
            "epoch  93 | loss  1.21 | perplexity     3.36| elapsed 785.81s \n",
            "epoch  94 | loss  1.20 | perplexity     3.32| elapsed 797.29s \n",
            "epoch  95 | loss  1.19 | perplexity     3.29| elapsed 808.98s \n",
            "epoch  96 | loss  1.18 | perplexity     3.27| elapsed 820.72s \n",
            "epoch  97 | loss  1.17 | perplexity     3.23| elapsed 832.37s \n",
            "epoch  98 | loss  1.16 | perplexity     3.20| elapsed 843.94s \n",
            "epoch  99 | loss  1.16 | perplexity     3.18| elapsed 855.60s \n",
            "epoch 100 | loss  1.15 | perplexity     3.15| elapsed 867.34s \n",
            "epoch 101 | loss  1.14 | perplexity     3.12| elapsed 878.89s \n",
            "epoch 102 | loss  1.14 | perplexity     3.11| elapsed 890.22s \n",
            "epoch 103 | loss  1.12 | perplexity     3.08| elapsed 901.97s \n",
            "epoch 104 | loss  1.12 | perplexity     3.05| elapsed 913.64s \n",
            "epoch 105 | loss  1.11 | perplexity     3.05| elapsed 925.30s \n",
            "epoch 106 | loss  1.10 | perplexity     3.00| elapsed 936.87s \n",
            "epoch 107 | loss  1.09 | perplexity     2.96| elapsed 948.40s \n",
            "epoch 108 | loss  1.09 | perplexity     2.96| elapsed 960.08s \n",
            "epoch 109 | loss  1.08 | perplexity     2.94| elapsed 971.54s \n",
            "epoch 110 | loss  1.07 | perplexity     2.92| elapsed 983.20s \n",
            "epoch 111 | loss  1.07 | perplexity     2.90| elapsed 994.86s \n",
            "epoch 112 | loss  1.06 | perplexity     2.88| elapsed 1006.60s \n",
            "epoch 113 | loss  1.05 | perplexity     2.86| elapsed 1018.25s \n",
            "epoch 114 | loss  1.05 | perplexity     2.85| elapsed 1030.03s \n",
            "epoch 115 | loss  1.04 | perplexity     2.83| elapsed 1041.48s \n",
            "epoch 116 | loss  1.04 | perplexity     2.82| elapsed 1053.01s \n",
            "epoch 117 | loss  1.03 | perplexity     2.79| elapsed 1064.48s \n",
            "epoch 118 | loss  1.02 | perplexity     2.79| elapsed 1076.22s \n",
            "epoch 119 | loss  1.02 | perplexity     2.77| elapsed 1087.89s \n",
            "epoch 120 | loss  1.01 | perplexity     2.75| elapsed 1099.54s \n",
            "epoch 121 | loss  1.00 | perplexity     2.72| elapsed 1111.05s \n",
            "epoch 122 | loss  1.00 | perplexity     2.71| elapsed 1122.69s \n",
            "epoch 123 | loss  0.99 | perplexity     2.70| elapsed 1134.29s \n",
            "epoch 124 | loss  0.99 | perplexity     2.68| elapsed 1145.99s \n",
            "epoch 125 | loss  0.98 | perplexity     2.67| elapsed 1157.60s \n",
            "epoch 126 | loss  0.98 | perplexity     2.66| elapsed 1169.19s \n",
            "epoch 127 | loss  0.96 | perplexity     2.62| elapsed 1180.68s \n",
            "epoch 128 | loss  0.96 | perplexity     2.61| elapsed 1192.43s \n",
            "epoch 129 | loss  0.96 | perplexity     2.62| elapsed 1204.05s \n",
            "epoch 130 | loss  0.95 | perplexity     2.59| elapsed 1215.60s \n",
            "epoch 131 | loss  0.95 | perplexity     2.57| elapsed 1227.23s \n",
            "epoch 132 | loss  0.95 | perplexity     2.57| elapsed 1238.67s \n",
            "epoch 133 | loss  0.94 | perplexity     2.56| elapsed 1250.30s \n",
            "epoch 134 | loss  0.93 | perplexity     2.54| elapsed 1262.04s \n",
            "epoch 135 | loss  0.93 | perplexity     2.53| elapsed 1273.78s \n",
            "epoch 136 | loss  0.92 | perplexity     2.52| elapsed 1285.63s \n",
            "epoch 137 | loss  0.92 | perplexity     2.51| elapsed 1297.36s \n",
            "epoch 138 | loss  0.91 | perplexity     2.49| elapsed 1308.99s \n",
            "epoch 139 | loss  0.91 | perplexity     2.49| elapsed 1320.60s \n",
            "epoch 140 | loss  0.90 | perplexity     2.46| elapsed 1332.22s \n",
            "epoch 141 | loss  0.90 | perplexity     2.46| elapsed 1343.74s \n",
            "epoch 142 | loss  0.89 | perplexity     2.44| elapsed 1355.47s \n",
            "epoch 143 | loss  0.89 | perplexity     2.45| elapsed 1366.98s \n",
            "epoch 144 | loss  0.89 | perplexity     2.43| elapsed 1378.66s \n",
            "epoch 145 | loss  0.89 | perplexity     2.43| elapsed 1390.32s \n",
            "epoch 146 | loss  0.88 | perplexity     2.42| elapsed 1401.96s \n",
            "epoch 147 | loss  0.88 | perplexity     2.41| elapsed 1413.69s \n",
            "epoch 148 | loss  0.87 | perplexity     2.39| elapsed 1425.32s \n",
            "epoch 149 | loss  0.87 | perplexity     2.38| elapsed 1437.08s \n",
            "epoch 150 | loss  0.86 | perplexity     2.37| elapsed 1448.50s \n",
            "Zipped files at /content/data/jozsef-wp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oueW91h7bjak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate verses! "
      ]
    },
    {
      "metadata": {
        "id": "vtS63EkMOIbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "import torch\n",
        "\n",
        "def is_unbalanced(s):\n",
        "  if s.count('\"') % 2 != 0 or s.count('(') != s.count(')'):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def sample_punkt():\n",
        "  return sample(['.', '?', '!'], 1)[0]\n",
        "\n",
        "def parse_last_line(s):\n",
        "  l = list(s)\n",
        "  \n",
        "  if l[-1] == ',': \n",
        "    l[-1] = sample_punkt()\n",
        "  if l[-1] not in list('.?!'): \n",
        "    l.append(sample_punkt())  \n",
        "  return \"\".join(l)\n",
        "  \n",
        "def generate_line(model, hidden=None, temp=1.0, \n",
        "               sos_id=1, eos_id=2, unk_id=0, max_len=None):\n",
        "  \"\"\"Generate line from `model` with `hidden` state at `temp`.\"\"\"\n",
        "  ids = []\n",
        "  \n",
        "  if hidden is None:\n",
        "    hidden = model.init_hidden(1)\n",
        "  \n",
        "  input = torch.tensor([sos_id], dtype=torch.long).reshape(1,1).to(device)\n",
        "  \n",
        "  id = 0\n",
        "  while id != eos_id and len(ids)<max_len :\n",
        "    output, hidden = model(input, hidden)\n",
        "    probs = output.squeeze().div(temp).exp().cpu() \n",
        "    id = torch.multinomial(probs, num_samples=1).item() \n",
        "    if id == sos_id or id == unk_id: continue\n",
        "    input.fill_(id)\n",
        "    ids += [id]\n",
        "  \n",
        "  return ids, hidden\n",
        "\n",
        "def generate(model, tokenizer, num_lines=8, min_len=8, max_len=15,\n",
        "             unk_id=0, sos_id=1, eos_id=2, temp=0.6):\n",
        "  \"\"\" \n",
        "  Generate a verse consisting of `num_lines` lines of max. length `max_tokens`.\n",
        "  Since the hidden state is passed onto the next line, \n",
        "  observing some cross-line consistency would be expected, or less\n",
        "  optimistically, at least grammatically correct sentences.\n",
        "  NOTE: line length can be tuned by changing max_tokens (i.e. subword pieces).\n",
        "  \n",
        "  Args:\n",
        "    model: Trained PyTorch language model\n",
        "    tokenizer: SentencePiece tokenizer\n",
        "    temp: Temperature parameter; lower: more conservative, higher: more diverse\n",
        "    num_lines: No. of lines to generate.\n",
        "    max_len: Max no. of tokens per line (not words!)\n",
        "    sos_id: Start of sequence id in vocabulary\n",
        "    eos_id: End of sequence id in vocabulary\n",
        "  \n",
        "  Returns: list of strings\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  \n",
        "  lines = []\n",
        "  line_cnt = 0\n",
        "  hidden = model.init_hidden(1)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    while line_cnt != num_lines:\n",
        "      try:\n",
        "\n",
        "        ids, hidden = generate_line(model, hidden=hidden, temp=temp, max_len=max_len,\n",
        "                                    sos_id=sos_id, eos_id=eos_id, unk_id=unk_id)\n",
        "        \n",
        "        if len(ids) <= min_len: raise Exception\n",
        "        line = tokenizer.textify(ids).strip()\n",
        "        \n",
        "        if line.startswith(tuple(\"-?!.,()\")): raise Exception\n",
        "        if is_unbalanced(line): raise Exception\n",
        "        \n",
        "        lines += [line]\n",
        "        line_cnt +=1\n",
        "        \n",
        "      except Exception as e:\n",
        "        pass\n",
        "    \n",
        "  last_line = lines.pop()\n",
        "  l = parse_last_line(last_line)\n",
        "  lines.append(l)\n",
        "  \n",
        "  return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73w_4CMz-Mrv",
        "colab_type": "code",
        "outputId": "db02e48e-8730-47c4-d4be-fa64a0da9952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.5, num_lines=10, min_len=8, max_len=14)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Űr a lelkem, a lét türelme.',\n",
              " 's a kicból a halál!',\n",
              " 'adás, a bérházon a tenger',\n",
              " 'a sokszor be nem al elé.',\n",
              " 'a kék kein belieber leser.',\n",
              " 's a kis öreg hosszan bámul az eltünő csapat után',\n",
              " 'meg erős, a tet nem fogom megcsókolni',\n",
              " 'a vágy, a jó és az ágy,',\n",
              " 'több a mi fehér kövér, ki ád',\n",
              " 'az Isten is - a hold, a csatorna felé fodorul!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "metadata": {
        "id": "9mCMMjXJQOQ-",
        "colab_type": "code",
        "outputId": "1f77b654-f717-40c7-89e8-93f185c283b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.6, num_lines=20, max_len=15)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Űr a lelkem. A tömegeken így lesz neki',\n",
              " 'kit rajta nem hallod, csak a kedves',\n",
              " 'a hajnal, a szó a csontunk',\n",
              " 'is látni magát a Szél, a Nap, a halál.',\n",
              " 'S a mult, a szó, a jog,',\n",
              " 'az asszony, amíg a munkás beleöltözött,',\n",
              " 'amíg de te megérkezik a meztelen leányzó.',\n",
              " 's a kék, az asszony is,',\n",
              " 's az a kétszert szerez:',\n",
              " 'a csönd, a halál nem rossz, én a hajat kedveltem mindig,',\n",
              " 'a bagoly hd, hogy kösse letöröm.',\n",
              " 's a kis öreg hosszan bámul az eltünő csapat után,',\n",
              " 'S a szép vágyig, ha nem tud ott ketté a faág,',\n",
              " 's hánynyolhonban a földtekén:',\n",
              " 'a vágy, de mi a Nap, a világ!',\n",
              " 'az asszony, a létra, a nap',\n",
              " 's a nagy városokat, ahol a gyümölcse, te e a föld,',\n",
              " 'az Isten is lesz, hogy több a pja,',\n",
              " 'a fény, mert a kés éle,',\n",
              " 'az a szép, hogy a fény meg!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "metadata": {
        "id": "_yNUM_HPSpvw",
        "colab_type": "code",
        "outputId": "4b7228ee-fa7f-4d3d-e5d0-049c5c80acce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.7, num_lines=10, max_len=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a föld, az ég, a csönd',\n",
              " 'a kő, a mi nem is,',\n",
              " 'csak egy darabos kontyba kötjük, s egy fény az ég.',\n",
              " 'a halak, mely ősz hull a fösvény országban, ahol',\n",
              " 'a multé, fehér habok fölött.',\n",
              " 'a mezei ház mellé, hol a kert bokrain túl',\n",
              " 'kenyeret, hogy húzódik fagyott s a fény,',\n",
              " 'a csont a csendjét a ház,',\n",
              " 'a légy, az idegeket, izmokat,',\n",
              " 'a kín, az idegem, hajnal harmat.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    }
  ]
}