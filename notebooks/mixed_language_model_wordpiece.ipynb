{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mixed-language-model-wordpiece.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben0it8/poetry-language-model/blob/master/mixed_language_model_wordpiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3Syp72_eY06e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Install extra requirements"
      ]
    },
    {
      "metadata": {
        "id": "EAGNkWy_D5er",
        "colab_type": "code",
        "outputId": "ea6e2c7e-c187-43f3-f93c-307e859eb21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U ftfy\n",
        "!pip install -U sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.5.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/8a/0e4a10bc00a0263db8d45d0062c83892598eb58e8091f439c63926e9b107/sentencepiece-0.1.81-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 13.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYlrr4nDEk5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'; # adapt plots for retina displays\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid');\n",
        "sns.set_context(context='notebook');\n",
        "from typing import List\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import numpy as np \n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import ftfy \n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import dill\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.nn import functional as F\n",
        "from random import sample\n",
        "\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "data_dir = Path(\"data/mixed\").resolve()\n",
        "data_dir.mkdir(exist_ok=True, parents=True)\n",
        "url = \"https://raw.githubusercontent.com/ben0it8/ady/master/data/merged.txt\" \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIaijzWl2G-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_paths(path):\n",
        "  path = Path(path)\n",
        "  ret = []\n",
        "  for x in path.glob('*'):\n",
        "    if x.is_file() and not str(x).endswith('.zip'):\n",
        "      ret.append(str(x))\n",
        "  return ret\n",
        "\n",
        "def zip_dir(path, name):\n",
        "  if not name.endswith('.zip'): name += '.zip'\n",
        "  file_paths = get_file_paths(path)\n",
        "  \n",
        "  with ZipFile(path/name, 'w') as zip:\n",
        "    for file in file_paths:\n",
        "      zip.write(file, os.path.basename(file))\n",
        "  print(f\"Zipped files at {path}\")\n",
        "  return (path/name).resolve()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIxEBl5Irzky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Read data"
      ]
    },
    {
      "metadata": {
        "id": "-HobfQaar-ID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean data & write to disk"
      ]
    },
    {
      "metadata": {
        "id": "LidRmCTeFDP-",
        "colab_type": "code",
        "outputId": "c9c75c68-46fa-4f2e-ec05-c261a6c378a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "def is_title(t):\n",
        "  t = re.sub('[\\d\\s\\s\\-!?,]', '', t)\n",
        "  return t.isupper()\n",
        "\n",
        "def fix_text(t:str):\n",
        "  t = ftfy.fix_text(t, normalization='NFKC')\n",
        "  t = t.replace('\\n', '') # remove newlines\n",
        "  t = re.sub(r'[»«]', '', t) # remove special parenthesis\n",
        "  t = re.sub(r'[0-9]','', t)\n",
        "  t = re.sub(\"\\s\\s+\", \" \", t) # skip whitespaces\n",
        "  t = t.strip()\n",
        "  return t\n",
        "    \n",
        "def fix_texts(texts:list):\n",
        "  out = []\n",
        "  for i, line in enumerate(texts):\n",
        "    if \"\\u2424\" in line:\n",
        "      line = line.split(\"\\u2424\")\n",
        "    elif '\\u2028' in line:\n",
        "      line = line.split('\\u2028')\n",
        "    elif '\\u000A' in line:\n",
        "      line = line.split('\\u000A')\n",
        "    else:\n",
        "      line = [line] \n",
        "    for t in line:\n",
        "      t = fix_text(t)\n",
        "      if (t is None or len(t.replace(' ', ''))<=3 or is_title(t) or\n",
        "          t.startswith(('.', ',', '?', '!', '-', ';')) or len(t) > 100):\n",
        "        continue\n",
        "      else:\n",
        "        out += [t]   \n",
        "  return out\n",
        "\n",
        "response =  requests.get(url)\n",
        "texts = [line.decode() for line in response.iter_lines()]\n",
        "print(f\"No. of lines: {len(texts)}\")\n",
        "clean_texts = fix_texts(texts)\n",
        "(data_dir/'text_clean.txt').open(mode='wt').writelines(f\"{line}\\n\" for line in clean_texts)\n",
        "print(f\"No. of lines: {len(clean_texts)}\")\n",
        "np.random.shuffle(clean_texts)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of lines: 163126\n",
            "No. of lines: 177393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-m1fCHxGySxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Tokenizer (which uses SentencePiece), Corpus ( data handler)"
      ]
    },
    {
      "metadata": {
        "id": "wGJipK5GmgsT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    \n",
        "    ID_UNK = 0\n",
        "    ID_SOS = 1\n",
        "    ID_EOS = 2\n",
        "    \n",
        "    def __init__(self, model_path:str):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.Load(str(model_path))\n",
        "        self.id_unk = self.sp.unk_id()\n",
        "        self.tk_unk = self.sp.IdToPiece(self.id_unk)\n",
        "        self.tk_sos = self.sp.IdToPiece(self.ID_SOS)\n",
        "        self.tk_eos = self.sp.IdToPiece(self.ID_EOS)\n",
        "        self.vocab_size = len(self.sp)\n",
        "        logger.info(f\"Initialized SentPieceProcessor from {model_path}\")\n",
        "    \n",
        "    def numericalize(self, tokens: List[str]) -> List[List[int]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        ids =  [self.sp.EncodeAsIds(s) for s in tokens]\n",
        "        if len(ids) == 1: ids=ids[0]\n",
        "        return ids\n",
        "\n",
        "    def piecify(self, tokens: List[str]) -> List[List[str]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        return [self.sp.EncodeAsPieces(s) for s in tokens]\n",
        "    \n",
        "    def textify(self, ids: List[int]) -> str:\n",
        "        if isinstance(ids, list) and isinstance(ids[0], np.generic): \n",
        "            ids = [int(x) for x in ids]\n",
        "        if not isinstance(ids, list) and not isinstance(ids[0], int):\n",
        "            raise TypeError(\"Argument `ids` has to be a list of integers.\")            \n",
        "        return self.sp.DecodeIds(ids)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_file(cls, input_file:str, output_path:str='default', vocab_size:int=16000, \n",
        "                  char_cov:float=1.0, model_type:str='unigram'):\n",
        "        \n",
        "        assert model_type in ['unigram', 'bpe', 'char', 'word']\n",
        "        assert 0 < char_cov <= 1\n",
        "        input_file = str(input_file)\n",
        "        output_file =  os.path.splitext(str(output_path))[0]\n",
        "        ext = '.model'\n",
        "        train_cmd = f\"--input={input_file} --model_prefix={output_file}\"\\\n",
        "                    f\" --vocab_size={vocab_size} --character_coverage={char_cov} --model_type={model_type}\"\n",
        "\n",
        "        logger.info(f\"Train command: {train_cmd}\")\n",
        "        logger.info(f\"Started training SentencePiece model...\")\n",
        "        ret = spm.SentencePieceTrainer.Train(train_cmd)\n",
        "        logger.info(f\"Exit code: {int(ret)}\")\n",
        "        return cls(output_file+ext)\n",
        "      \n",
        "def batchify(data, bsz):\n",
        "    # work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "class Corpus(object):\n",
        "\n",
        "  def __init__(self, file_path, tokenizer, bs=20):\n",
        "       \n",
        "        self.processor = tokenizer\n",
        "        self.id_sos, self.id_eos = self.processor.ID_SOS, self.processor.ID_EOS\n",
        "        self.bs = bs\n",
        "        self.data = self.tokenize(file_path)        \n",
        "        self.vocab_size = self.processor.vocab_size\n",
        "        \n",
        "  def tokenize(self, path):\n",
        "      \"\"\"Tokenizes a text file.\"\"\"\n",
        "      ids = []\n",
        "      with open(path, 'r') as f:\n",
        "          for line in f:\n",
        "              numericalized = self.processor.numericalize(line)\n",
        "              ids.extend([self.id_sos] + numericalized + [self.id_eos])\n",
        "\n",
        "      return batchify(torch.LongTensor(ids), self.bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTn7LTQfkJlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define RNN model architecture,  training loop and helpers"
      ]
    },
    {
      "metadata": {
        "id": "bi951Z9uzzCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, emsize, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, emsize)\n",
        "        assert (rnn_type in ['LSTM', 'GRU']), \"Arg `rnn_type` has to be one of {GRU, LSTM}.\"\n",
        "        self.rnn = getattr(nn, rnn_type)(emsize, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != emsize:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "       \n",
        "      \n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "  if isinstance(h, torch.Tensor):\n",
        "    return h.detach()\n",
        "  else:\n",
        "    return tuple(repackage_hidden(v) for v in h)\n",
        "      \n",
        "def get_num_params(model):\n",
        "  return sum([p.nelement() for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "def get_batch(source, i, bptt):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "  \n",
        "def train_epoch(train_data, model, vocab_size, bs=16, bptt=20, clip=.25):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    hidden = model.init_hidden(bs)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        total_loss += len(data) * loss.item()\n",
        "        \n",
        "    return total_loss / (len(train_data) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w62PRkgBJS7W",
        "colab_type": "code",
        "outputId": "d1edf52a-8493-41d5-a301-00899ab2d4e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "# INIT AND TRAIN TOKENIZER FROM CLEAN TEXTS\n",
        "bs = 64\n",
        "\n",
        "model_type = 'unigram' # can be \"bpe\"/\"unigram\" to support wordpieces\n",
        "\n",
        "vocab_size = 22000 # how many wordpieces to consider\n",
        "\n",
        "tokenizer = Tokenizer.from_file(data_dir/'text_clean.txt', \n",
        "                                output_path=data_dir/'tokenizer', char_cov=1.0,\n",
        "                                model_type=model_type, vocab_size=vocab_size) \n",
        "\n",
        "# INIT CORPUS FROM CLEAN TEXTS AND TOKENIZER WITH BATCH_SIZE `BS`\n",
        "\n",
        "corpus = Corpus(data_dir/'text_clean.txt', tokenizer=tokenizer, bs=bs)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-10 15:49:19,385 : INFO : Train command: --input=/content/data/mixed/text_clean.txt --model_prefix=/content/data/mixed/tokenizer --vocab_size=22000 --character_coverage=1.0 --model_type=unigram\n",
            "2019-04-10 15:49:19,388 : INFO : Started training SentencePiece model...\n",
            "2019-04-10 15:49:55,676 : INFO : Exit code: 1\n",
            "2019-04-10 15:49:55,749 : INFO : Initialized SentPieceProcessor from /content/data/mixed/tokenizer.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3BDcrSMzjNP",
        "colab_type": "code",
        "outputId": "92b1bd7a-0398-41dd-8670-0f86e3ecc4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# LSTM PARAMETERS\n",
        "model_type='GRU'\n",
        "\n",
        "emsize = 600\n",
        "nhid = 600\n",
        "\n",
        "nlayers = 1\n",
        "\n",
        "dropout = 0.05\n",
        "clip = 3.5\n",
        "\n",
        "tied = True\n",
        "\n",
        "bptt = 80\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "# INIT LSTM MODEL, LOSS FUNCTION AND OPTIMIZER\n",
        "\n",
        "model = RNNModel(model_type, corpus.vocab_size, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "print(f\"No. of parameters: {get_num_params(model)}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No. of parameters: 15385600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FXRCWjOZaBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's train the model"
      ]
    },
    {
      "metadata": {
        "id": "H3zNzPST0eYI",
        "colab_type": "code",
        "outputId": "bc68e49c-1569-4360-cc8f-a3ccec699844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "  \n",
        "  for epoch in range(1, epochs+1):\n",
        "\n",
        "    loss = train_epoch(corpus.data, model, corpus.vocab_size, \n",
        "                       clip=clip, bs=bs, bptt=bptt)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"epoch {:3d} | loss {:5.2f} | perplexity {:8.2f}| elapsed {:5.2f}s \".format(epoch, loss, math.exp(loss), elapsed))\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print('-' * 89)\n",
        "  print('Exiting from training early')\n",
        "\n",
        "finally:\n",
        "  params = {\"model_type\": model_type,\n",
        "            \"ntoken\": corpus.vocab_size,\n",
        "            \"emsize\": emsize,\n",
        "            \"nhid\": nhid,\n",
        "            \"nlayers\": nlayers,\n",
        "            \"dropout\": dropout,\n",
        "            \"tied\": tied}\n",
        " \n",
        "  with open(data_dir/'model_state.pth', 'wb') as f:\n",
        "    torch.save({\"state_dict\": model.state_dict(),\n",
        "                \"params\": params}, f)\n",
        "    \n",
        "zipfile = zip_dir(data_dir, data_dir.name)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | loss  5.89 | perplexity   360.75| elapsed 98.40s \n",
            "epoch   2 | loss  5.24 | perplexity   187.93| elapsed 196.78s \n",
            "epoch   3 | loss  4.90 | perplexity   134.45| elapsed 295.31s \n",
            "epoch   4 | loss  4.65 | perplexity   104.25| elapsed 393.90s \n",
            "epoch   5 | loss  4.43 | perplexity    83.63| elapsed 492.40s \n",
            "epoch   6 | loss  4.22 | perplexity    68.05| elapsed 590.94s \n",
            "epoch   7 | loss  4.03 | perplexity    56.03| elapsed 689.58s \n",
            "epoch   8 | loss  3.84 | perplexity    46.47| elapsed 788.13s \n",
            "epoch   9 | loss  3.66 | perplexity    38.94| elapsed 886.69s \n",
            "epoch  10 | loss  3.50 | perplexity    33.07| elapsed 985.27s \n",
            "epoch  11 | loss  3.35 | perplexity    28.38| elapsed 1083.89s \n",
            "epoch  12 | loss  3.20 | perplexity    24.54| elapsed 1182.39s \n",
            "epoch  13 | loss  3.07 | perplexity    21.49| elapsed 1280.95s \n",
            "epoch  14 | loss  2.95 | perplexity    19.09| elapsed 1379.50s \n",
            "epoch  15 | loss  2.84 | perplexity    17.18| elapsed 1478.02s \n",
            "epoch  16 | loss  2.75 | perplexity    15.64| elapsed 1576.48s \n",
            "epoch  17 | loss  2.66 | perplexity    14.37| elapsed 1675.01s \n",
            "epoch  18 | loss  2.59 | perplexity    13.33| elapsed 1773.51s \n",
            "epoch  19 | loss  2.52 | perplexity    12.42| elapsed 1871.95s \n",
            "epoch  20 | loss  2.46 | perplexity    11.65| elapsed 1970.48s \n",
            "epoch  21 | loss  2.40 | perplexity    10.98| elapsed 2069.01s \n",
            "epoch  22 | loss  2.34 | perplexity    10.41| elapsed 2167.52s \n",
            "epoch  23 | loss  2.29 | perplexity     9.91| elapsed 2265.93s \n",
            "epoch  24 | loss  2.25 | perplexity     9.51| elapsed 2364.41s \n",
            "epoch  25 | loss  2.21 | perplexity     9.15| elapsed 2462.96s \n",
            "epoch  26 | loss  2.18 | perplexity     8.82| elapsed 2561.35s \n",
            "epoch  27 | loss  2.14 | perplexity     8.51| elapsed 2659.80s \n",
            "epoch  28 | loss  2.11 | perplexity     8.28| elapsed 2758.27s \n",
            "epoch  29 | loss  2.08 | perplexity     8.02| elapsed 2856.64s \n",
            "epoch  30 | loss  2.05 | perplexity     7.77| elapsed 2954.94s \n",
            "epoch  31 | loss  2.02 | perplexity     7.54| elapsed 3053.26s \n",
            "epoch  32 | loss  2.00 | perplexity     7.36| elapsed 3151.61s \n",
            "epoch  33 | loss  1.97 | perplexity     7.18| elapsed 3249.88s \n",
            "epoch  34 | loss  1.95 | perplexity     7.00| elapsed 3348.12s \n",
            "epoch  35 | loss  1.92 | perplexity     6.83| elapsed 3446.38s \n",
            "epoch  36 | loss  1.90 | perplexity     6.69| elapsed 3544.63s \n",
            "epoch  37 | loss  1.88 | perplexity     6.55| elapsed 3642.81s \n",
            "epoch  38 | loss  1.86 | perplexity     6.42| elapsed 3741.02s \n",
            "epoch  39 | loss  1.84 | perplexity     6.31| elapsed 3839.31s \n",
            "epoch  40 | loss  1.82 | perplexity     6.20| elapsed 3937.52s \n",
            "epoch  41 | loss  1.81 | perplexity     6.09| elapsed 4035.80s \n",
            "epoch  42 | loss  1.79 | perplexity     6.00| elapsed 4134.12s \n",
            "epoch  43 | loss  1.77 | perplexity     5.90| elapsed 4232.33s \n",
            "epoch  44 | loss  1.76 | perplexity     5.81| elapsed 4330.48s \n",
            "epoch  45 | loss  1.75 | perplexity     5.74| elapsed 4428.76s \n",
            "epoch  46 | loss  1.74 | perplexity     5.67| elapsed 4527.06s \n",
            "epoch  47 | loss  1.72 | perplexity     5.59| elapsed 4625.21s \n",
            "epoch  48 | loss  1.71 | perplexity     5.52| elapsed 4723.40s \n",
            "epoch  49 | loss  1.69 | perplexity     5.45| elapsed 4821.60s \n",
            "epoch  50 | loss  1.68 | perplexity     5.39| elapsed 4919.73s \n",
            "epoch  51 | loss  1.67 | perplexity     5.32| elapsed 5017.88s \n",
            "epoch  52 | loss  1.66 | perplexity     5.26| elapsed 5116.11s \n",
            "epoch  53 | loss  1.65 | perplexity     5.20| elapsed 5214.32s \n",
            "epoch  54 | loss  1.64 | perplexity     5.15| elapsed 5312.43s \n",
            "epoch  55 | loss  1.63 | perplexity     5.09| elapsed 5410.60s \n",
            "epoch  56 | loss  1.62 | perplexity     5.05| elapsed 5508.75s \n",
            "epoch  57 | loss  1.61 | perplexity     5.00| elapsed 5606.90s \n",
            "epoch  58 | loss  1.60 | perplexity     4.95| elapsed 5704.93s \n",
            "epoch  59 | loss  1.59 | perplexity     4.89| elapsed 5803.05s \n",
            "epoch  60 | loss  1.58 | perplexity     4.84| elapsed 5901.15s \n",
            "epoch  61 | loss  1.57 | perplexity     4.81| elapsed 5999.30s \n",
            "epoch  62 | loss  1.56 | perplexity     4.76| elapsed 6097.35s \n",
            "epoch  63 | loss  1.55 | perplexity     4.71| elapsed 6195.41s \n",
            "epoch  64 | loss  1.54 | perplexity     4.68| elapsed 6293.43s \n",
            "epoch  65 | loss  1.53 | perplexity     4.63| elapsed 6391.43s \n",
            "epoch  66 | loss  1.53 | perplexity     4.61| elapsed 6489.51s \n",
            "epoch  67 | loss  1.52 | perplexity     4.56| elapsed 6587.55s \n",
            "epoch  68 | loss  1.51 | perplexity     4.52| elapsed 6685.54s \n",
            "epoch  69 | loss  1.50 | perplexity     4.49| elapsed 6783.58s \n",
            "epoch  70 | loss  1.49 | perplexity     4.45| elapsed 6881.59s \n",
            "epoch  71 | loss  1.49 | perplexity     4.42| elapsed 6979.59s \n",
            "epoch  72 | loss  1.48 | perplexity     4.39| elapsed 7077.61s \n",
            "epoch  73 | loss  1.47 | perplexity     4.34| elapsed 7175.56s \n",
            "epoch  74 | loss  1.46 | perplexity     4.31| elapsed 7273.52s \n",
            "epoch  75 | loss  1.46 | perplexity     4.29| elapsed 7371.50s \n",
            "epoch  76 | loss  1.45 | perplexity     4.26| elapsed 7469.46s \n",
            "epoch  77 | loss  1.44 | perplexity     4.23| elapsed 7567.43s \n",
            "epoch  78 | loss  1.43 | perplexity     4.20| elapsed 7665.43s \n",
            "epoch  79 | loss  1.43 | perplexity     4.16| elapsed 7763.35s \n",
            "epoch  80 | loss  1.42 | perplexity     4.15| elapsed 7861.35s \n",
            "epoch  81 | loss  1.42 | perplexity     4.12| elapsed 7959.38s \n",
            "epoch  82 | loss  1.41 | perplexity     4.09| elapsed 8057.28s \n",
            "epoch  83 | loss  1.40 | perplexity     4.06| elapsed 8155.25s \n",
            "epoch  84 | loss  1.40 | perplexity     4.05| elapsed 8253.21s \n",
            "epoch  85 | loss  1.39 | perplexity     4.02| elapsed 8351.15s \n",
            "epoch  86 | loss  1.38 | perplexity     3.99| elapsed 8449.07s \n",
            "epoch  87 | loss  1.38 | perplexity     3.97| elapsed 8547.07s \n",
            "epoch  88 | loss  1.37 | perplexity     3.95| elapsed 8645.05s \n",
            "epoch  89 | loss  1.37 | perplexity     3.92| elapsed 8743.01s \n",
            "epoch  90 | loss  1.37 | perplexity     3.92| elapsed 8840.98s \n",
            "epoch  91 | loss  1.36 | perplexity     3.89| elapsed 8938.98s \n",
            "epoch  92 | loss  1.35 | perplexity     3.87| elapsed 9036.92s \n",
            "epoch  93 | loss  1.35 | perplexity     3.84| elapsed 9134.87s \n",
            "epoch  94 | loss  1.34 | perplexity     3.81| elapsed 9232.85s \n",
            "epoch  95 | loss  1.33 | perplexity     3.79| elapsed 9330.87s \n",
            "epoch  96 | loss  1.33 | perplexity     3.78| elapsed 9428.85s \n",
            "epoch  97 | loss  1.33 | perplexity     3.77| elapsed 9526.80s \n",
            "epoch  98 | loss  1.32 | perplexity     3.75| elapsed 9624.76s \n",
            "epoch  99 | loss  1.31 | perplexity     3.72| elapsed 9722.69s \n",
            "epoch 100 | loss  1.31 | perplexity     3.71| elapsed 9820.58s \n",
            "Zipped files at /content/data/mixed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oueW91h7bjak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate verses! "
      ]
    },
    {
      "metadata": {
        "id": "vtS63EkMOIbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "import torch\n",
        "\n",
        "def is_unbalanced(s):\n",
        "  if s.count('\"') % 2 != 0 or s.count('(') != s.count(')'):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def sample_punkt():\n",
        "  return sample(['.', '?', '!'], 1)[0]\n",
        "\n",
        "def parse_last_line(s):\n",
        "  l = list(s)\n",
        "  if l[-1] == ',': \n",
        "    l[-1] = sample_punkt()\n",
        "  \n",
        "  if l[-1] not in list('.?!'): \n",
        "    l.append(sample_punkt())  \n",
        "  return \"\".join(l)\n",
        "  \n",
        "def generate_line(model, hidden=None, temp=1.0, \n",
        "               sos_id=1, eos_id=2, unk_id=0, max_len=None):\n",
        "  \"\"\"Generate line from `model` with `hidden` state at `temp`.\"\"\"\n",
        "  ids = []\n",
        "  \n",
        "  if hidden is None:\n",
        "    hidden = model.init_hidden(1)\n",
        "  \n",
        "  input = torch.tensor([sos_id], dtype=torch.long).reshape(1,1).to(device)\n",
        "  \n",
        "  id = 0\n",
        "  while id != eos_id and len(ids)<max_len :\n",
        "    output, hidden = model(input, hidden)\n",
        "    probs = output.squeeze().div(temp).exp().cpu() \n",
        "    id = torch.multinomial(probs, num_samples=1).item() \n",
        "    if id == sos_id or id == unk_id: continue\n",
        "    input.fill_(id)\n",
        "    ids += [id]\n",
        "  \n",
        "  return ids, hidden\n",
        "\n",
        "def generate(model, tokenizer, num_lines=8, min_len=8, max_len=15,\n",
        "             unk_id=0, sos_id=1, eos_id=2, temp=0.6):\n",
        "  \"\"\" \n",
        "  Generate a verse consisting of `num_lines` lines of max. length `max_tokens`.\n",
        "  Since the hidden state is passed onto the next line, \n",
        "  observing some cross-line consistency would be expected, or less\n",
        "  optimistically, at least grammatically correct sentences.\n",
        "  NOTE: line length can be tuned by changing max_tokens (i.e. subword pieces).\n",
        "  \n",
        "  Args:\n",
        "    model: Trained PyTorch language model\n",
        "    tokenizer: SentencePiece tokenizer\n",
        "    temp: Temperature parameter; lower: more conservative, higher: more diverse\n",
        "    num_lines: No. of lines to generate.\n",
        "    max_len: Max no. of tokens per line (not words!)\n",
        "    sos_id: Start of sequence id in vocabulary\n",
        "    eos_id: End of sequence id in vocabulary\n",
        "  \n",
        "  Returns: list of strings\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  \n",
        "  lines = []\n",
        "  line_cnt = 0\n",
        "  hidden = model.init_hidden(1)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    while line_cnt != num_lines:\n",
        "      try:\n",
        "\n",
        "        ids, hidden = generate_line(model, hidden=hidden, temp=temp, max_len=max_len,\n",
        "                                    sos_id=sos_id, eos_id=eos_id, unk_id=unk_id)\n",
        "        \n",
        "        if len(ids) <= min_len: raise Exception\n",
        "        line = tokenizer.textify(ids).strip()\n",
        "        \n",
        "        if line.startswith(tuple(\"-?!.,()\")): raise Exception\n",
        "        if is_unbalanced(line): raise Exception\n",
        "        \n",
        "        lines += [line]\n",
        "        line_cnt +=1\n",
        "        \n",
        "      except Exception as e:\n",
        "        pass\n",
        "    \n",
        "  last_line = lines.pop()\n",
        "  l = parse_last_line(last_line)\n",
        "  lines.append(l)\n",
        "  \n",
        "  return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73w_4CMz-Mrv",
        "colab_type": "code",
        "outputId": "74b906a3-c79b-4fb5-aa00-ce5a2a537125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.4, num_lines=10, min_len=8, max_len=12)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Minden, ami a miénk lesz az a jövevény,',\n",
              " 'a jég, mely a kék és sötét műhelyekben',\n",
              " 'és az árnyékmá meg a tiszta kékségben',\n",
              " 'a nap s a torony, az óriás, merev tekintetű',\n",
              " 'a parasztságra véres késsel a fölcskét,',\n",
              " 'a boldog vágy, de nem gondol senkire, ki látja',\n",
              " 'a szabad kőre követni kívánnak.',\n",
              " 's az igazság itt minden igazság bugyognak benne.',\n",
              " 'a harctér s a tavasz-tis',\n",
              " 'a munka, ez a penészes végzet.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "9mCMMjXJQOQ-",
        "colab_type": "code",
        "outputId": "0f906777-548c-4e1f-bd2c-890ab69da728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.6, num_lines=8, max_len=15)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a Határ és a bevezetés.',\n",
              " 'a jéghidegöst, az istent és a halált.',\n",
              " 'a lány és nem volt kérdem - -',\n",
              " 's az illyes deszkák alatt,',\n",
              " 'a munka s az én derék a Puffadok alatt,',\n",
              " 'a leglangodott a világ,',\n",
              " 'és az örök mozgás, amely egy igazság szerint,',\n",
              " 'és a nevetésben, amint föl-föl is aszna hordja magát?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "metadata": {
        "id": "_yNUM_HPSpvw",
        "colab_type": "code",
        "outputId": "8020743d-ca7e-4774-b503-a1d67110458b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.7, num_lines=10, max_len=15)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arra kérd a vezér: \"Mit beszélsz? Rém unalmas én vagyok?\"',\n",
              " 'a te ispocs-hang, a színház',\n",
              " 'a lány. Az én fülemüket ontják',\n",
              " 'né a maga szövőszékeken s a',\n",
              " 'a vörös dúb-kuvál',\n",
              " 'a vöröses és alagsori,',\n",
              " 'a salmiák és a kiváncsiak.',\n",
              " 'a tanszék támogatja a gazdag teljesedést.',\n",
              " 'az élet a nótának remélhet.',\n",
              " 'a munka, de a meglibos jéggel a!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "metadata": {
        "id": "0FUgYTowNNZ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}