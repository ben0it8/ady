{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kassak-language-model-wordpiece.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben0it8/poetry-language-model/blob/master/kassak_language_model_wordpiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jpXivGD0YpHn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Check GPU memory availability"
      ]
    },
    {
      "metadata": {
        "id": "NVEc6D3Tlsnl",
        "colab_type": "code",
        "outputId": "207a8ec6-735c-4ba0-ea1d-99fad42f87b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 249.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Syp72_eY06e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Install extra requirements"
      ]
    },
    {
      "metadata": {
        "id": "EAGNkWy_D5er",
        "colab_type": "code",
        "outputId": "ac5a3517-1de8-445c-d9a1-3ce1c91b46e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U ftfy\n",
        "!pip install -U sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.5.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/8a/0e4a10bc00a0263db8d45d0062c83892598eb58e8091f439c63926e9b107/sentencepiece-0.1.81-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 18.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYlrr4nDEk5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'; # adapt plots for retina displays\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid');\n",
        "sns.set_context(context='notebook');\n",
        "from typing import List\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import numpy as np \n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import ftfy \n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import dill\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.nn import functional as F\n",
        "from random import sample\n",
        "\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "data_dir = Path(\"data/kassak-wp\").resolve()\n",
        "data_dir.mkdir(exist_ok=True, parents=True)\n",
        "url = \"https://raw.githubusercontent.com/ben0it8/ady/master/data/kassak.txt\" \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIaijzWl2G-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_paths(path):\n",
        "  path = Path(path)\n",
        "  ret = []\n",
        "  for x in path.glob('*'):\n",
        "    if x.is_file() and not str(x).endswith('.zip'):\n",
        "      ret.append(str(x))\n",
        "  return ret\n",
        "\n",
        "def zip_dir(path, name):\n",
        "  if not name.endswith('.zip'): name += '.zip'\n",
        "  file_paths = get_file_paths(path)\n",
        "  \n",
        "  with ZipFile(path/name, 'w') as zip:\n",
        "    for file in file_paths:\n",
        "      zip.write(file, os.path.basename(file))\n",
        "  print(f\"Zipped files at {path}\")\n",
        "  return (path/name).resolve()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIxEBl5Irzky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Read data"
      ]
    },
    {
      "metadata": {
        "id": "-HobfQaar-ID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean data & write to disk"
      ]
    },
    {
      "metadata": {
        "id": "LidRmCTeFDP-",
        "colab_type": "code",
        "outputId": "e7c0e823-eda6-40b8-91f3-f42828cc981a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def is_title(t):\n",
        "  t = re.sub('[\\d\\s\\s\\-!?,]', '', t)\n",
        "  return t.isupper()\n",
        "\n",
        "def fix_text(t:str):\n",
        "  t = ftfy.fix_text(t, normalization='NFKC')\n",
        "  t = t.replace('\\n', '') # remove newlines\n",
        "  t = re.sub(r'[»«]', '', t) # remove special parenthesis\n",
        "  t = re.sub(r'[0-9]','', t)\n",
        "  t = re.sub(\"\\s\\s+\", \" \", t) # skip whitespaces\n",
        "  t = t.strip()\n",
        "  return t\n",
        "    \n",
        "def fix_texts(texts:list):\n",
        "  out = []\n",
        "  for i, t in enumerate(texts):\n",
        "    t = fix_text(t)\n",
        "    if t is None or len(t)<=2 or is_title(t):\n",
        "      continue\n",
        "    else: out += [t]   \n",
        "  return out\n",
        "response =  requests.get(url)\n",
        "texts = [line.decode() for line in response.iter_lines()]\n",
        "print(f\"No. of lines: {len(texts)}\")\n",
        "clean_texts = fix_texts(texts)\n",
        "(data_dir/'text_clean.txt').open(mode='wt').writelines(f\"{line}\\n\" for line in clean_texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of lines: 40911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-m1fCHxGySxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Tokenizer (which uses SentencePiece), Corpus ( data handler)"
      ]
    },
    {
      "metadata": {
        "id": "wGJipK5GmgsT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    \n",
        "    ID_UNK = 0\n",
        "    ID_SOS = 1\n",
        "    ID_EOS = 2\n",
        "    \n",
        "    def __init__(self, model_path:str):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.Load(str(model_path))\n",
        "        self.id_unk = self.sp.unk_id()\n",
        "        self.tk_unk = self.sp.IdToPiece(self.id_unk)\n",
        "        self.tk_sos = self.sp.IdToPiece(self.ID_SOS)\n",
        "        self.tk_eos = self.sp.IdToPiece(self.ID_EOS)\n",
        "        self.vocab_size = len(self.sp)\n",
        "        logger.info(f\"Initialized SentPieceProcessor from {model_path}\")\n",
        "    \n",
        "    def numericalize(self, tokens: List[str]) -> List[List[int]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        ids =  [self.sp.EncodeAsIds(s) for s in tokens]\n",
        "        if len(ids) == 1: ids=ids[0]\n",
        "        return ids\n",
        "\n",
        "    def piecify(self, tokens: List[str]) -> List[List[str]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        return [self.sp.EncodeAsPieces(s) for s in tokens]\n",
        "    \n",
        "    def textify(self, ids: List[int]) -> str:\n",
        "        if isinstance(ids, list) and isinstance(ids[0], np.generic): \n",
        "            ids = [int(x) for x in ids]\n",
        "        if not isinstance(ids, list) and not isinstance(ids[0], int):\n",
        "            raise TypeError(\"Argument `ids` has to be a list of integers.\")            \n",
        "        return self.sp.DecodeIds(ids)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_file(cls, input_file:str, output_path:str='default', vocab_size:int=16000, \n",
        "                  char_cov:float=1.0, model_type:str='unigram'):\n",
        "        \n",
        "        assert model_type in ['unigram', 'bpe', 'char', 'word']\n",
        "        assert 0 < char_cov <= 1\n",
        "        input_file = str(input_file)\n",
        "        output_file =  os.path.splitext(str(output_path))[0]\n",
        "        ext = '.model'\n",
        "        train_cmd = f\"--input={input_file} --model_prefix={output_file}\"\\\n",
        "                    f\" --vocab_size={vocab_size} --character_coverage={char_cov} --model_type={model_type}\"\n",
        "\n",
        "        logger.info(f\"Train command: {train_cmd}\")\n",
        "        logger.info(f\"Started training SentencePiece model...\")\n",
        "        ret = spm.SentencePieceTrainer.Train(train_cmd)\n",
        "        logger.info(f\"Exit code: {int(ret)}\")\n",
        "        return cls(output_file+ext)\n",
        "      \n",
        "def batchify(data, bsz):\n",
        "    # work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "class Corpus(object):\n",
        "\n",
        "  def __init__(self, file_path, tokenizer, bs=20):\n",
        "       \n",
        "        self.processor = tokenizer\n",
        "        self.id_sos, self.id_eos = self.processor.ID_SOS, self.processor.ID_EOS\n",
        "        self.bs = bs\n",
        "        self.data = self.tokenize(file_path)        \n",
        "        self.vocab_size = self.processor.vocab_size\n",
        "        \n",
        "  def tokenize(self, path):\n",
        "      \"\"\"Tokenizes a text file.\"\"\"\n",
        "      ids = []\n",
        "      with open(path, 'r') as f:\n",
        "          for line in f:\n",
        "              numericalized = self.processor.numericalize(line)\n",
        "              ids.extend([self.id_sos] + numericalized + [self.id_eos])\n",
        "\n",
        "      return batchify(torch.LongTensor(ids), self.bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTn7LTQfkJlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define RNN model architecture,  training loop and helpers"
      ]
    },
    {
      "metadata": {
        "id": "bi951Z9uzzCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, emsize, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, emsize)\n",
        "        assert (rnn_type in ['LSTM', 'GRU']), \"Arg `rnn_type` has to be one of {GRU, LSTM}.\"\n",
        "        self.rnn = getattr(nn, rnn_type)(emsize, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != emsize:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "       \n",
        "      \n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "  if isinstance(h, torch.Tensor):\n",
        "    return h.detach()\n",
        "  else:\n",
        "    return tuple(repackage_hidden(v) for v in h)\n",
        "      \n",
        "def get_num_params(model):\n",
        "  return sum([p.nelement() for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "def get_batch(source, i, bptt):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "  \n",
        "def train_epoch(train_data, model, vocab_size, bs=16, bptt=20, clip=.25):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    hidden = model.init_hidden(bs)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        total_loss += len(data) * loss.item()\n",
        "        \n",
        "    return total_loss / (len(train_data) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w62PRkgBJS7W",
        "colab_type": "code",
        "outputId": "63c024fc-9b05-4835-fe40-94ce820c29d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "# INIT AND TRAIN TOKENIZER FROM CLEAN TEXTS\n",
        "bs = 64\n",
        "\n",
        "model_type = 'unigram' # can be \"bpe\"/\"unigram\" to support wordpieces\n",
        "\n",
        "vocab_size = 16000 # how many wordpieces to consider\n",
        "\n",
        "tokenizer = Tokenizer.from_file(data_dir/'text_clean.txt', \n",
        "                                output_path=data_dir/'tokenizer', char_cov=1.0,\n",
        "                                model_type=model_type, vocab_size=vocab_size) \n",
        "\n",
        "# INIT CORPUS FROM CLEAN TEXTS AND TOKENIZER WITH BATCH_SIZE `BS`\n",
        "\n",
        "corpus = Corpus(data_dir/'text_clean.txt', tokenizer=tokenizer, bs=bs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-09 12:26:07,328 : INFO : Train command: --input=/content/data/kassak-wp/text_clean.txt --model_prefix=/content/data/kassak-wp/tokenizer --vocab_size=16000 --character_coverage=1.0 --model_type=unigram\n",
            "2019-04-09 12:26:07,337 : INFO : Started training SentencePiece model...\n",
            "2019-04-09 12:26:22,027 : INFO : Exit code: 1\n",
            "2019-04-09 12:26:22,195 : INFO : Initialized SentPieceProcessor from /content/data/kassak-wp/tokenizer.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3BDcrSMzjNP",
        "colab_type": "code",
        "outputId": "b8f78290-3bff-4a15-a84e-b5cacce6ea23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# LSTM PARAMETERS\n",
        "model_type='GRU'\n",
        "\n",
        "emsize = 500\n",
        "nhid = 500\n",
        "\n",
        "nlayers = 1\n",
        "\n",
        "dropout = 0.1\n",
        "clip = 5.0\n",
        "\n",
        "tied = True\n",
        "\n",
        "bptt = 80\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "# INIT LSTM MODEL, LOSS FUNCTION AND OPTIMIZER\n",
        "\n",
        "model = RNNModel(model_type, corpus.vocab_size, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "print(f\"No. of parameters: {get_num_params(model)}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No. of parameters: 9519000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FXRCWjOZaBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's train the model"
      ]
    },
    {
      "metadata": {
        "id": "H3zNzPST0eYI",
        "colab_type": "code",
        "outputId": "53f4803c-4514-4500-c6f7-7f76e26aa841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2584
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 150\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "  \n",
        "  for epoch in range(1, epochs+1):\n",
        "\n",
        "    loss = train_epoch(corpus.data, model, corpus.vocab_size, \n",
        "                       clip=clip, bs=bs, bptt=bptt)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"epoch {:3d} | loss {:5.2f} | perplexity {:8.2f}| elapsed {:5.2f}s \".format(epoch, loss, math.exp(loss), elapsed))\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print('-' * 89)\n",
        "  print('Exiting from training early')\n",
        "\n",
        "finally:\n",
        "  params = {\"model_type\": model_type,\n",
        "            \"ntoken\": corpus.vocab_size,\n",
        "            \"emsize\": emsize,\n",
        "            \"nhid\": nhid,\n",
        "            \"nlayers\": nlayers,\n",
        "            \"dropout\": dropout,\n",
        "            \"tied\": tied}\n",
        " \n",
        "  with open(data_dir/'model_state.pth', 'wb') as f:\n",
        "    torch.save({\"state_dict\": model.state_dict(),\n",
        "                \"params\": params}, f)\n",
        "    \n",
        "zipfile = zip_dir(data_dir, data_dir.name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | loss  6.66 | perplexity   782.57| elapsed 45.14s \n",
            "epoch   2 | loss  5.95 | perplexity   382.27| elapsed 89.96s \n",
            "epoch   3 | loss  5.68 | perplexity   292.98| elapsed 135.01s \n",
            "epoch   4 | loss  5.46 | perplexity   235.30| elapsed 179.97s \n",
            "epoch   5 | loss  5.27 | perplexity   194.69| elapsed 219.99s \n",
            "epoch   6 | loss  5.09 | perplexity   162.98| elapsed 259.65s \n",
            "epoch   7 | loss  4.92 | perplexity   137.25| elapsed 306.78s \n",
            "epoch   8 | loss  4.75 | perplexity   115.38| elapsed 355.16s \n",
            "epoch   9 | loss  4.58 | perplexity    97.22| elapsed 404.27s \n",
            "epoch  10 | loss  4.41 | perplexity    81.95| elapsed 453.40s \n",
            "epoch  11 | loss  4.24 | perplexity    69.50| elapsed 502.36s \n",
            "epoch  12 | loss  4.08 | perplexity    59.21| elapsed 551.00s \n",
            "epoch  13 | loss  3.92 | perplexity    50.45| elapsed 599.99s \n",
            "epoch  14 | loss  3.76 | perplexity    43.09| elapsed 649.01s \n",
            "epoch  15 | loss  3.61 | perplexity    36.83| elapsed 698.00s \n",
            "epoch  16 | loss  3.45 | perplexity    31.53| elapsed 746.80s \n",
            "epoch  17 | loss  3.31 | perplexity    27.35| elapsed 795.87s \n",
            "epoch  18 | loss  3.17 | perplexity    23.84| elapsed 845.15s \n",
            "epoch  19 | loss  3.04 | perplexity    20.96| elapsed 893.87s \n",
            "epoch  20 | loss  2.92 | perplexity    18.46| elapsed 943.06s \n",
            "epoch  21 | loss  2.79 | perplexity    16.32| elapsed 992.03s \n",
            "epoch  22 | loss  2.68 | perplexity    14.59| elapsed 1040.95s \n",
            "epoch  23 | loss  2.57 | perplexity    13.07| elapsed 1090.07s \n",
            "epoch  24 | loss  2.47 | perplexity    11.81| elapsed 1136.62s \n",
            "epoch  25 | loss  2.37 | perplexity    10.70| elapsed 1176.62s \n",
            "epoch  26 | loss  2.27 | perplexity     9.69| elapsed 1217.05s \n",
            "epoch  27 | loss  2.18 | perplexity     8.83| elapsed 1256.98s \n",
            "epoch  28 | loss  2.09 | perplexity     8.09| elapsed 1297.09s \n",
            "epoch  29 | loss  2.02 | perplexity     7.51| elapsed 1337.69s \n",
            "epoch  30 | loss  1.94 | perplexity     6.99| elapsed 1377.20s \n",
            "epoch  31 | loss  1.88 | perplexity     6.56| elapsed 1417.03s \n",
            "epoch  32 | loss  1.82 | perplexity     6.16| elapsed 1457.21s \n",
            "epoch  33 | loss  1.76 | perplexity     5.82| elapsed 1497.23s \n",
            "epoch  34 | loss  1.71 | perplexity     5.55| elapsed 1537.11s \n",
            "epoch  35 | loss  1.67 | perplexity     5.33| elapsed 1576.73s \n",
            "epoch  36 | loss  1.63 | perplexity     5.12| elapsed 1616.83s \n",
            "epoch  37 | loss  1.58 | perplexity     4.87| elapsed 1657.29s \n",
            "epoch  38 | loss  1.53 | perplexity     4.63| elapsed 1697.56s \n",
            "epoch  39 | loss  1.48 | perplexity     4.41| elapsed 1736.85s \n",
            "epoch  40 | loss  1.44 | perplexity     4.22| elapsed 1776.86s \n",
            "epoch  41 | loss  1.40 | perplexity     4.05| elapsed 1817.15s \n",
            "epoch  42 | loss  1.35 | perplexity     3.86| elapsed 1857.08s \n",
            "epoch  43 | loss  1.31 | perplexity     3.71| elapsed 1897.56s \n",
            "epoch  44 | loss  1.28 | perplexity     3.58| elapsed 1937.82s \n",
            "epoch  45 | loss  1.24 | perplexity     3.47| elapsed 1977.85s \n",
            "epoch  46 | loss  1.22 | perplexity     3.37| elapsed 2017.34s \n",
            "epoch  47 | loss  1.19 | perplexity     3.28| elapsed 2057.75s \n",
            "epoch  48 | loss  1.16 | perplexity     3.20| elapsed 2094.70s \n",
            "epoch  49 | loss  1.13 | perplexity     3.11| elapsed 2131.48s \n",
            "epoch  50 | loss  1.11 | perplexity     3.03| elapsed 2168.16s \n",
            "epoch  51 | loss  1.09 | perplexity     2.96| elapsed 2204.58s \n",
            "epoch  52 | loss  1.07 | perplexity     2.90| elapsed 2241.31s \n",
            "epoch  53 | loss  1.05 | perplexity     2.85| elapsed 2278.17s \n",
            "epoch  54 | loss  1.03 | perplexity     2.80| elapsed 2314.96s \n",
            "epoch  55 | loss  1.01 | perplexity     2.75| elapsed 2351.63s \n",
            "epoch  56 | loss  0.99 | perplexity     2.70| elapsed 2388.43s \n",
            "epoch  57 | loss  0.98 | perplexity     2.65| elapsed 2424.95s \n",
            "epoch  58 | loss  0.96 | perplexity     2.62| elapsed 2461.60s \n",
            "epoch  59 | loss  0.95 | perplexity     2.58| elapsed 2497.78s \n",
            "epoch  60 | loss  0.93 | perplexity     2.55| elapsed 2534.28s \n",
            "epoch  61 | loss  0.92 | perplexity     2.51| elapsed 2570.89s \n",
            "epoch  62 | loss  0.91 | perplexity     2.49| elapsed 2607.36s \n",
            "epoch  63 | loss  0.90 | perplexity     2.46| elapsed 2643.81s \n",
            "epoch  64 | loss  0.89 | perplexity     2.44| elapsed 2680.44s \n",
            "epoch  65 | loss  0.88 | perplexity     2.41| elapsed 2717.00s \n",
            "epoch  66 | loss  0.87 | perplexity     2.39| elapsed 2753.48s \n",
            "epoch  67 | loss  0.86 | perplexity     2.36| elapsed 2777.82s \n",
            "epoch  68 | loss  0.85 | perplexity     2.33| elapsed 2800.16s \n",
            "epoch  69 | loss  0.84 | perplexity     2.31| elapsed 2816.17s \n",
            "epoch  70 | loss  0.82 | perplexity     2.27| elapsed 2832.24s \n",
            "epoch  71 | loss  0.81 | perplexity     2.24| elapsed 2848.30s \n",
            "epoch  72 | loss  0.80 | perplexity     2.22| elapsed 2864.36s \n",
            "epoch  73 | loss  0.79 | perplexity     2.20| elapsed 2880.41s \n",
            "epoch  74 | loss  0.78 | perplexity     2.18| elapsed 2896.45s \n",
            "epoch  75 | loss  0.76 | perplexity     2.15| elapsed 2912.51s \n",
            "epoch  76 | loss  0.76 | perplexity     2.13| elapsed 2928.56s \n",
            "epoch  77 | loss  0.75 | perplexity     2.11| elapsed 2944.61s \n",
            "epoch  78 | loss  0.74 | perplexity     2.09| elapsed 2960.66s \n",
            "epoch  79 | loss  0.73 | perplexity     2.08| elapsed 2976.71s \n",
            "epoch  80 | loss  0.72 | perplexity     2.06| elapsed 2992.75s \n",
            "epoch  81 | loss  0.71 | perplexity     2.04| elapsed 3008.81s \n",
            "epoch  82 | loss  0.71 | perplexity     2.03| elapsed 3024.87s \n",
            "epoch  83 | loss  0.70 | perplexity     2.01| elapsed 3040.91s \n",
            "epoch  84 | loss  0.69 | perplexity     1.99| elapsed 3056.96s \n",
            "epoch  85 | loss  0.69 | perplexity     1.98| elapsed 3073.01s \n",
            "epoch  86 | loss  0.67 | perplexity     1.96| elapsed 3089.08s \n",
            "epoch  87 | loss  0.67 | perplexity     1.95| elapsed 3105.12s \n",
            "epoch  88 | loss  0.66 | perplexity     1.94| elapsed 3121.18s \n",
            "epoch  89 | loss  0.65 | perplexity     1.92| elapsed 3137.32s \n",
            "epoch  90 | loss  0.64 | perplexity     1.90| elapsed 3153.45s \n",
            "epoch  91 | loss  0.64 | perplexity     1.89| elapsed 3169.62s \n",
            "epoch  92 | loss  0.63 | perplexity     1.88| elapsed 3185.87s \n",
            "epoch  93 | loss  0.62 | perplexity     1.86| elapsed 3201.94s \n",
            "epoch  94 | loss  0.62 | perplexity     1.85| elapsed 3217.99s \n",
            "epoch  95 | loss  0.61 | perplexity     1.84| elapsed 3234.04s \n",
            "epoch  96 | loss  0.60 | perplexity     1.83| elapsed 3257.99s \n",
            "epoch  97 | loss  0.60 | perplexity     1.82| elapsed 3284.26s \n",
            "epoch  98 | loss  0.59 | perplexity     1.81| elapsed 3310.58s \n",
            "epoch  99 | loss  0.59 | perplexity     1.80| elapsed 3336.72s \n",
            "epoch 100 | loss  0.58 | perplexity     1.78| elapsed 3362.86s \n",
            "epoch 101 | loss  0.58 | perplexity     1.78| elapsed 3388.84s \n",
            "epoch 102 | loss  0.57 | perplexity     1.77| elapsed 3416.69s \n",
            "epoch 103 | loss  0.56 | perplexity     1.76| elapsed 3448.18s \n",
            "epoch 104 | loss  0.56 | perplexity     1.75| elapsed 3483.90s \n",
            "epoch 105 | loss  0.55 | perplexity     1.74| elapsed 3519.50s \n",
            "epoch 106 | loss  0.55 | perplexity     1.73| elapsed 3554.66s \n",
            "epoch 107 | loss  0.55 | perplexity     1.72| elapsed 3590.33s \n",
            "epoch 108 | loss  0.54 | perplexity     1.72| elapsed 3625.70s \n",
            "epoch 109 | loss  0.54 | perplexity     1.71| elapsed 3661.51s \n",
            "epoch 110 | loss  0.53 | perplexity     1.70| elapsed 3696.81s \n",
            "epoch 111 | loss  0.53 | perplexity     1.69| elapsed 3732.12s \n",
            "epoch 112 | loss  0.52 | perplexity     1.68| elapsed 3767.53s \n",
            "epoch 113 | loss  0.51 | perplexity     1.67| elapsed 3802.77s \n",
            "epoch 114 | loss  0.51 | perplexity     1.67| elapsed 3837.51s \n",
            "epoch 115 | loss  0.51 | perplexity     1.66| elapsed 3873.25s \n",
            "epoch 116 | loss  0.50 | perplexity     1.65| elapsed 3908.59s \n",
            "epoch 117 | loss  0.50 | perplexity     1.64| elapsed 3944.06s \n",
            "epoch 118 | loss  0.49 | perplexity     1.64| elapsed 3979.31s \n",
            "epoch 119 | loss  0.49 | perplexity     1.63| elapsed 4014.71s \n",
            "epoch 120 | loss  0.49 | perplexity     1.63| elapsed 4050.11s \n",
            "epoch 121 | loss  0.48 | perplexity     1.62| elapsed 4085.34s \n",
            "epoch 122 | loss  0.48 | perplexity     1.61| elapsed 4120.65s \n",
            "epoch 123 | loss  0.47 | perplexity     1.61| elapsed 4156.25s \n",
            "epoch 124 | loss  0.47 | perplexity     1.60| elapsed 4191.38s \n",
            "epoch 125 | loss  0.47 | perplexity     1.59| elapsed 4226.67s \n",
            "epoch 126 | loss  0.46 | perplexity     1.59| elapsed 4261.88s \n",
            "epoch 127 | loss  0.46 | perplexity     1.58| elapsed 4297.11s \n",
            "epoch 128 | loss  0.46 | perplexity     1.58| elapsed 4332.16s \n",
            "epoch 129 | loss  0.45 | perplexity     1.57| elapsed 4365.49s \n",
            "epoch 130 | loss  0.45 | perplexity     1.57| elapsed 4392.22s \n",
            "epoch 131 | loss  0.45 | perplexity     1.56| elapsed 4418.96s \n",
            "epoch 132 | loss  0.44 | perplexity     1.56| elapsed 4445.69s \n",
            "epoch 133 | loss  0.44 | perplexity     1.55| elapsed 4472.41s \n",
            "epoch 134 | loss  0.44 | perplexity     1.55| elapsed 4499.23s \n",
            "epoch 135 | loss  0.43 | perplexity     1.54| elapsed 4525.91s \n",
            "epoch 136 | loss  0.43 | perplexity     1.54| elapsed 4552.59s \n",
            "epoch 137 | loss  0.43 | perplexity     1.53| elapsed 4579.25s \n",
            "epoch 138 | loss  0.42 | perplexity     1.53| elapsed 4605.92s \n",
            "epoch 139 | loss  0.42 | perplexity     1.52| elapsed 4632.69s \n",
            "epoch 140 | loss  0.42 | perplexity     1.52| elapsed 4659.41s \n",
            "epoch 141 | loss  0.42 | perplexity     1.52| elapsed 4686.13s \n",
            "epoch 142 | loss  0.41 | perplexity     1.51| elapsed 4712.79s \n",
            "epoch 143 | loss  0.41 | perplexity     1.50| elapsed 4739.51s \n",
            "epoch 144 | loss  0.41 | perplexity     1.50| elapsed 4766.16s \n",
            "epoch 145 | loss  0.40 | perplexity     1.50| elapsed 4792.81s \n",
            "epoch 146 | loss  0.40 | perplexity     1.49| elapsed 4819.58s \n",
            "epoch 147 | loss  0.40 | perplexity     1.49| elapsed 4846.20s \n",
            "epoch 148 | loss  0.40 | perplexity     1.49| elapsed 4872.83s \n",
            "epoch 149 | loss  0.39 | perplexity     1.48| elapsed 4899.48s \n",
            "epoch 150 | loss  0.39 | perplexity     1.48| elapsed 4926.13s \n",
            "Zipped files at /content/data/kassak-wp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oueW91h7bjak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate verses! "
      ]
    },
    {
      "metadata": {
        "id": "vtS63EkMOIbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "import torch\n",
        "\n",
        "def is_unbalanced(s):\n",
        "  if s.count('\"') % 2 != 0 or s.count('(') != s.count(')'):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def sample_punkt():\n",
        "  return sample(['.', '?', '!'], 1)[0]\n",
        "\n",
        "def parse_last_line(s):\n",
        "  l = list(s)\n",
        "  \n",
        "  if l[-1] == ',': \n",
        "    l[-1] = sample_punkt()\n",
        "  \n",
        "  if l[-1] not in list('.?!'): \n",
        "    l.append(sample_punkt())  \n",
        "  return \"\".join(l)\n",
        "  \n",
        "def generate_line(model, hidden=None, temp=1.0, \n",
        "               sos_id=1, eos_id=2, unk_id=0, max_len=None):\n",
        "  \"\"\"Generate line from `model` with `hidden` state at `temp`.\"\"\"\n",
        "  ids = []\n",
        "  \n",
        "  if hidden is None:\n",
        "    hidden = model.init_hidden(1)\n",
        "  \n",
        "  input = torch.tensor([sos_id], dtype=torch.long).reshape(1,1).to(device)\n",
        "  \n",
        "  id = 0\n",
        "  while id != eos_id and len(ids)<max_len :\n",
        "    output, hidden = model(input, hidden)\n",
        "    probs = output.squeeze().div(temp).exp().cpu() \n",
        "    id = torch.multinomial(probs, num_samples=1).item() \n",
        "    if id == sos_id or id == unk_id: continue\n",
        "    input.fill_(id)\n",
        "    ids += [id]\n",
        "  \n",
        "  return ids, hidden\n",
        "\n",
        "def generate(model, tokenizer, num_lines=8, min_len=8, max_len=15,\n",
        "             unk_id=0, sos_id=1, eos_id=2, temp=0.6):\n",
        "  \"\"\" \n",
        "  Generate a verse consisting of `num_lines` lines of max. length `max_tokens`.\n",
        "  Since the hidden state is passed onto the next line, \n",
        "  observing some cross-line consistency would be expected, or less\n",
        "  optimistically, at least grammatically correct sentences.\n",
        "  NOTE: line length can be tuned by changing max_tokens (i.e. subword pieces).\n",
        "  \n",
        "  Args:\n",
        "    model: Trained PyTorch language model\n",
        "    tokenizer: SentencePiece tokenizer\n",
        "    temp: Temperature parameter; lower: more conservative, higher: more diverse\n",
        "    num_lines: No. of lines to generate.\n",
        "    max_len: Max no. of tokens per line (not words!)\n",
        "    sos_id: Start of sequence id in vocabulary\n",
        "    eos_id: End of sequence id in vocabulary\n",
        "  \n",
        "  Returns: list of strings\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  \n",
        "  lines = []\n",
        "  line_cnt = 0\n",
        "  hidden = model.init_hidden(1)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    while line_cnt != num_lines:\n",
        "      try:\n",
        "\n",
        "        ids, hidden = generate_line(model, hidden=hidden, temp=temp, max_len=max_len,\n",
        "                                    sos_id=sos_id, eos_id=eos_id, unk_id=unk_id)\n",
        "        \n",
        "        if len(ids) <= min_len: raise Exception\n",
        "        line = tokenizer.textify(ids).strip()\n",
        "        \n",
        "        if line.startswith(tuple(\"-?!.,()\")): raise Exception\n",
        "        if is_unbalanced(line): raise Exception\n",
        "        \n",
        "        lines += [line]\n",
        "        line_cnt +=1\n",
        "        \n",
        "      except Exception as e:\n",
        "        pass\n",
        "    \n",
        "  last_line = lines.pop()\n",
        "  l = parse_last_line(last_line)\n",
        "  lines.append(l)\n",
        "  \n",
        "  return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73w_4CMz-Mrv",
        "colab_type": "code",
        "outputId": "4db00ef6-7b5e-476d-bca1-82a2b9ecd31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.4, num_lines=20, min_len=8, max_len=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3467ab9c9292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9mCMMjXJQOQ-",
        "colab_type": "code",
        "outputId": "fa830fe4-09cd-4edb-bbc9-e7d72bfbca96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.6, num_lines=20, min_len=8, max_len=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bólogat az eperfa, zöld árnyéka is bólogat',\n",
              " 'fölsárulni a nagy történelmi vásznakat és jó ki',\n",
              " 'a nagy öröm ahol a te példaadó életedhez',\n",
              " 'a kehesz fölfonott papa körül',\n",
              " 'kemény mézeskalácsot osztott el közöttük',\n",
              " 'a nagy hídfúllapokba',\n",
              " 'a ti listáitokat Koterünket nyomát sem ismernéd meg a káposzt',\n",
              " 'néha elvonulnak a hangok s a madarak röpte',\n",
              " 'a nedves bőr mint a majúzott asszonyok',\n",
              " 'üvegből nem szakadhat el a legkevultat és a nehéz föld alatt',\n",
              " 'a megtisztították esszékolni',\n",
              " 'kinyitjuk a tenyerünket és arótalan emberek akik szeretnek engem',\n",
              " 'ki fogja meg belőlem kimerfarokhoz',\n",
              " 'üvegestől vérszínű cselédeket csikorgott az',\n",
              " 'az idő szövő felejtette s a fiatal vörösné már',\n",
              " 'a meghasonlás is mintha terzzukk',\n",
              " 'a levágott ökrök segíthetnének de ők mélyen alszanak az áruházak',\n",
              " 'panaszkodó nem ezek a kiválasztottak előtt végül is föl a fölijedt költő népem',\n",
              " 'ha valaki a semmi kétségbe a tulajdon erőnk az ő aki lelépte át',\n",
              " 'ruhátok ezek a dolgok amiknek értelme a templomban!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "_yNUM_HPSpvw",
        "colab_type": "code",
        "outputId": "343ca680-e696-425a-8a3f-c75124de7709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.7, num_lines=20, min_len=8, max_len=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['egyszerűsága és a bírák keserűvel.',\n",
              " 'a mérleg nyelve a szétpiros rózsa fénykörében',\n",
              " 'színészei felé és sokat imádkozik a késüket.',\n",
              " 'üvegesetlen vagy s a fiatal vörös szép fiatal lány',\n",
              " 's a megrálisk mint a kiéhezett ragadozók.',\n",
              " 'a tanítványokat is emlékszemtuk meg a példa.',\n",
              " 'a levágott ökrök segítést adott át a szegényeket.',\n",
              " 'kemény hóval ivott meg a jó és szegények',\n",
              " 'az a fölbírzott a Önyát.',\n",
              " 'a szakállas emberhez a csíkospeketonnak',\n",
              " 'ormoló cilinderből és a telefonba',\n",
              " 'valami kis csikó a kirosúk szeretik a drapériák',\n",
              " 's ezért nem tudom lelopottan áll a nyomdász, akik',\n",
              " 'a megalázkodás igéit hirdetném aországról',\n",
              " 'a békáról és nagy vörösem felé',\n",
              " 'a Graciokió gyerekeké akit most sem tudott fölkelni',\n",
              " 'a nagy térben amiben nem fogni melletted',\n",
              " 'a te vagy tegyde meg a',\n",
              " 'a hevült mérleg hántánkat szerettük volna',\n",
              " 'a hős ami nem fog csírát ivott ki?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}