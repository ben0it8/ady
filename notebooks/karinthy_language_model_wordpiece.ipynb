{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "karinthy-language-model-wordpiece.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben0it8/poetry-language-model/blob/master/karinthy_language_model_wordpiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jpXivGD0YpHn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Check GPU memory availability"
      ]
    },
    {
      "metadata": {
        "id": "NVEc6D3Tlsnl",
        "colab_type": "code",
        "outputId": "207a8ec6-735c-4ba0-ea1d-99fad42f87b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 249.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Syp72_eY06e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Install extra requirements"
      ]
    },
    {
      "metadata": {
        "id": "EAGNkWy_D5er",
        "colab_type": "code",
        "outputId": "ac5a3517-1de8-445c-d9a1-3ce1c91b46e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U ftfy\n",
        "!pip install -U sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.5.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/8a/0e4a10bc00a0263db8d45d0062c83892598eb58e8091f439c63926e9b107/sentencepiece-0.1.81-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 18.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYlrr4nDEk5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'; # adapt plots for retina displays\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid');\n",
        "sns.set_context(context='notebook');\n",
        "from typing import List\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import numpy as np \n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import ftfy \n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import dill\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.nn import functional as F\n",
        "from random import sample\n",
        "\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "data_dir = Path(\"data/karinthy-wp\").resolve()\n",
        "data_dir.mkdir(exist_ok=True, parents=True)\n",
        "url = \"https://raw.githubusercontent.com/ben0it8/ady/master/data/karinthy.txt\" \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIaijzWl2G-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_paths(path):\n",
        "  path = Path(path)\n",
        "  ret = []\n",
        "  for x in path.glob('*'):\n",
        "    if x.is_file() and not str(x).endswith('.zip'):\n",
        "      ret.append(str(x))\n",
        "  return ret\n",
        "\n",
        "def zip_dir(path, name):\n",
        "  if not name.endswith('.zip'): name += '.zip'\n",
        "  file_paths = get_file_paths(path)\n",
        "  \n",
        "  with ZipFile(path/name, 'w') as zip:\n",
        "    for file in file_paths:\n",
        "      zip.write(file, os.path.basename(file))\n",
        "  print(f\"Zipped files at {path}\")\n",
        "  return (path/name).resolve()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIxEBl5Irzky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Read data"
      ]
    },
    {
      "metadata": {
        "id": "-HobfQaar-ID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean data & write to disk"
      ]
    },
    {
      "metadata": {
        "id": "LidRmCTeFDP-",
        "colab_type": "code",
        "outputId": "17052088-7ab7-41fc-aedf-0795e85d6708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "def is_title(t):\n",
        "  t = re.sub('[\\d\\s\\s\\-!?,]', '', t)\n",
        "  return t.isupper()\n",
        "\n",
        "def fix_text(t:str):\n",
        "  t = ftfy.fix_text(t, normalization='NFKC')\n",
        "  t = t.replace('\\n', '') # remove newlines\n",
        "  t = re.sub(r'[»«]', '', t) # remove special parenthesis\n",
        "  t = re.sub(r'[0-9]','', t)\n",
        "  t = re.sub(\"\\s\\s+\", \" \", t) # skip whitespaces\n",
        "  t = t.strip()\n",
        "  return t\n",
        "    \n",
        "def fix_texts(texts:list):\n",
        "  out = []\n",
        "  for i, line in enumerate(texts):\n",
        "    if \"\\u2424\" in line:\n",
        "      line = line.split(\"\\u2424\")\n",
        "    elif '\\u2028' in line:\n",
        "      line = line.split('\\u2028')\n",
        "    elif '\\u000A' in line:\n",
        "      line = line.split('\\u000A')\n",
        "    else:\n",
        "      line = [line] \n",
        "    for t in line:\n",
        "      t = fix_text(t)\n",
        "      if (t is None or len(t.replace(' ', ''))<=3 or is_title(t) or\n",
        "          t.startswith(('.', ',', '?', '!', '-', ';')) or len(t) > 100):\n",
        "        continue\n",
        "      else:\n",
        "        out += [t]   \n",
        "  return out\n",
        "\n",
        "\n",
        "response =  requests.get(url)\n",
        "texts = [line.decode() for line in response.iter_lines()]\n",
        "print(f\"No. of lines: {len(texts)}\")\n",
        "clean_texts = fix_texts(texts)\n",
        "print(f\"No. of clean lines: {len(clean_texts)}\")\n",
        "(data_dir/'text_clean.txt').open(mode='wt').writelines(f\"{line}\\n\" for line in clean_texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of lines: 783\n",
            "No. of clean lines: 5185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-m1fCHxGySxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Tokenizer (which uses SentencePiece), Corpus ( data handler)"
      ]
    },
    {
      "metadata": {
        "id": "wGJipK5GmgsT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    \n",
        "    ID_UNK = 0\n",
        "    ID_SOS = 1\n",
        "    ID_EOS = 2\n",
        "    \n",
        "    def __init__(self, model_path:str):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.Load(str(model_path))\n",
        "        self.id_unk = self.sp.unk_id()\n",
        "        self.tk_unk = self.sp.IdToPiece(self.id_unk)\n",
        "        self.tk_sos = self.sp.IdToPiece(self.ID_SOS)\n",
        "        self.tk_eos = self.sp.IdToPiece(self.ID_EOS)\n",
        "        self.vocab_size = len(self.sp)\n",
        "        logger.info(f\"Initialized SentPieceProcessor from {model_path}\")\n",
        "    \n",
        "    def numericalize(self, tokens: List[str]) -> List[List[int]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        ids =  [self.sp.EncodeAsIds(s) for s in tokens]\n",
        "        if len(ids) == 1: ids=ids[0]\n",
        "        return ids\n",
        "\n",
        "    def piecify(self, tokens: List[str]) -> List[List[str]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        return [self.sp.EncodeAsPieces(s) for s in tokens]\n",
        "    \n",
        "    def textify(self, ids: List[int]) -> str:\n",
        "        if isinstance(ids, list) and isinstance(ids[0], np.generic): \n",
        "            ids = [int(x) for x in ids]\n",
        "        if not isinstance(ids, list) and not isinstance(ids[0], int):\n",
        "            raise TypeError(\"Argument `ids` has to be a list of integers.\")            \n",
        "        return self.sp.DecodeIds(ids)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_file(cls, input_file:str, output_path:str='default', vocab_size:int=16000, \n",
        "                  char_cov:float=1.0, model_type:str='unigram'):\n",
        "        \n",
        "        assert model_type in ['unigram', 'bpe', 'char', 'word']\n",
        "        assert 0 < char_cov <= 1\n",
        "        input_file = str(input_file)\n",
        "        output_file =  os.path.splitext(str(output_path))[0]\n",
        "        ext = '.model'\n",
        "        train_cmd = f\"--input={input_file} --model_prefix={output_file}\"\\\n",
        "                    f\" --vocab_size={vocab_size} --character_coverage={char_cov} --model_type={model_type}\"\n",
        "\n",
        "        logger.info(f\"Train command: {train_cmd}\")\n",
        "        logger.info(f\"Started training SentencePiece model...\")\n",
        "        ret = spm.SentencePieceTrainer.Train(train_cmd)\n",
        "        logger.info(f\"Exit code: {int(ret)}\")\n",
        "        return cls(output_file+ext)\n",
        "      \n",
        "def batchify(data, bsz):\n",
        "    # work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "class Corpus(object):\n",
        "\n",
        "  def __init__(self, file_path, tokenizer, bs=20):\n",
        "       \n",
        "        self.processor = tokenizer\n",
        "        self.id_sos, self.id_eos = self.processor.ID_SOS, self.processor.ID_EOS\n",
        "        self.bs = bs\n",
        "        self.data = self.tokenize(file_path)        \n",
        "        self.vocab_size = self.processor.vocab_size\n",
        "        \n",
        "  def tokenize(self, path):\n",
        "      \"\"\"Tokenizes a text file.\"\"\"\n",
        "      ids = []\n",
        "      with open(path, 'r') as f:\n",
        "          for line in f:\n",
        "              numericalized = self.processor.numericalize(line)\n",
        "              ids.extend([self.id_sos] + numericalized + [self.id_eos])\n",
        "\n",
        "      return batchify(torch.LongTensor(ids), self.bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTn7LTQfkJlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define RNN model architecture,  training loop and helpers"
      ]
    },
    {
      "metadata": {
        "id": "bi951Z9uzzCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, emsize, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, emsize)\n",
        "        assert (rnn_type in ['LSTM', 'GRU']), \"Arg `rnn_type` has to be one of {GRU, LSTM}.\"\n",
        "        self.rnn = getattr(nn, rnn_type)(emsize, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != emsize:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "       \n",
        "      \n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "  if isinstance(h, torch.Tensor):\n",
        "    return h.detach()\n",
        "  else:\n",
        "    return tuple(repackage_hidden(v) for v in h)\n",
        "      \n",
        "def get_num_params(model):\n",
        "  return sum([p.nelement() for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "def get_batch(source, i, bptt):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "  \n",
        "def train_epoch(train_data, model, vocab_size, bs=16, bptt=20, clip=.25):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    hidden = model.init_hidden(bs)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        if clip:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        total_loss += len(data) * loss.item()\n",
        "        \n",
        "    return total_loss / (len(train_data) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w62PRkgBJS7W",
        "colab_type": "code",
        "outputId": "a2c51f81-52eb-439a-b4ff-876fdab3241b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "# INIT AND TRAIN TOKENIZER FROM CLEAN TEXTS\n",
        "bs = 32\n",
        "\n",
        "model_type = 'bpe' # can be \"bpe\"/\"unigram\" to support wordpieces\n",
        "\n",
        "vocab_size = 8000 # how many wordpieces to consider\n",
        "\n",
        "tokenizer = Tokenizer.from_file(data_dir/'text_clean.txt', \n",
        "                                output_path=data_dir/'tokenizer', char_cov=1.0,\n",
        "                                model_type=model_type, vocab_size=vocab_size) \n",
        "\n",
        "# INIT CORPUS FROM CLEAN TEXTS AND TOKENIZER WITH BATCH_SIZE `BS`\n",
        "\n",
        "corpus = Corpus(data_dir/'text_clean.txt', tokenizer=tokenizer, bs=bs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-10 13:22:26,618 : INFO : Train command: --input=/content/data/karinthy-wp/text_clean.txt --model_prefix=/content/data/karinthy-wp/tokenizer --vocab_size=10000 --character_coverage=1.0 --model_type=bpe\n",
            "2019-04-10 13:22:26,632 : INFO : Started training SentencePiece model...\n",
            "2019-04-10 13:22:29,914 : INFO : Exit code: 1\n",
            "2019-04-10 13:22:29,929 : INFO : Initialized SentPieceProcessor from /content/data/karinthy-wp/tokenizer.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3BDcrSMzjNP",
        "colab_type": "code",
        "outputId": "dea2da51-6806-480c-a50d-7027e1eeef0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "# LSTM PARAMETERS\n",
        "model_type='GRU'\n",
        "\n",
        "emsize = 256\n",
        "nhid = 256\n",
        "\n",
        "nlayers = 1\n",
        "\n",
        "dropout = 0.2\n",
        "clip = 3.0\n",
        "\n",
        "tied = True\n",
        "\n",
        "bptt = 80\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "# INIT LSTM MODEL, LOSS FUNCTION AND OPTIMIZER\n",
        "\n",
        "model = RNNModel(model_type, corpus.vocab_size, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "print(f\"No. of parameters: {get_num_params(model)}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of parameters: 2964752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FXRCWjOZaBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's train the model"
      ]
    },
    {
      "metadata": {
        "id": "H3zNzPST0eYI",
        "colab_type": "code",
        "outputId": "e4b90be7-1deb-40cf-818d-a64e9774db37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "  \n",
        "  for epoch in range(1, epochs+1):\n",
        "\n",
        "    loss = train_epoch(corpus.data, model, corpus.vocab_size, \n",
        "                       clip=clip, bs=bs, bptt=bptt)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"epoch {:3d} | loss {:5.2f} | perplexity {:8.2f}| elapsed {:5.2f}s \".format(epoch, loss, math.exp(loss), elapsed))\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print('-' * 89)\n",
        "  print('Exiting from training early')\n",
        "\n",
        "finally:\n",
        "  params = {\"model_type\": model_type,\n",
        "            \"ntoken\": corpus.vocab_size,\n",
        "            \"emsize\": emsize,\n",
        "            \"nhid\": nhid,\n",
        "            \"nlayers\": nlayers,\n",
        "            \"dropout\": dropout,\n",
        "            \"tied\": tied}\n",
        " \n",
        "  with open(data_dir/'model_state.pth', 'wb') as f:\n",
        "    torch.save({\"state_dict\": model.state_dict(),\n",
        "                \"params\": params}, f)\n",
        "    \n",
        "zipfile = zip_dir(data_dir, data_dir.name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | loss  8.10 | perplexity  3304.67| elapsed  2.13s \n",
            "epoch   2 | loss  6.81 | perplexity   903.35| elapsed  4.29s \n",
            "epoch   3 | loss  6.57 | perplexity   714.21| elapsed  6.43s \n",
            "epoch   4 | loss  6.44 | perplexity   623.80| elapsed  8.53s \n",
            "epoch   5 | loss  6.31 | perplexity   552.61| elapsed 10.67s \n",
            "epoch   6 | loss  6.22 | perplexity   502.81| elapsed 12.80s \n",
            "epoch   7 | loss  6.14 | perplexity   463.10| elapsed 14.90s \n",
            "epoch   8 | loss  6.06 | perplexity   430.16| elapsed 17.04s \n",
            "epoch   9 | loss  5.99 | perplexity   400.26| elapsed 19.19s \n",
            "epoch  10 | loss  5.92 | perplexity   371.65| elapsed 21.33s \n",
            "epoch  11 | loss  5.84 | perplexity   343.64| elapsed 23.43s \n",
            "epoch  12 | loss  5.76 | perplexity   317.01| elapsed 25.69s \n",
            "epoch  13 | loss  5.68 | perplexity   294.28| elapsed 27.79s \n",
            "epoch  14 | loss  5.60 | perplexity   271.09| elapsed 30.01s \n",
            "epoch  15 | loss  5.52 | perplexity   250.28| elapsed 32.16s \n",
            "epoch  16 | loss  5.44 | perplexity   230.61| elapsed 34.36s \n",
            "epoch  17 | loss  5.36 | perplexity   212.63| elapsed 36.47s \n",
            "epoch  18 | loss  5.28 | perplexity   195.67| elapsed 38.59s \n",
            "epoch  19 | loss  5.20 | perplexity   180.49| elapsed 40.72s \n",
            "epoch  20 | loss  5.12 | perplexity   167.14| elapsed 42.86s \n",
            "epoch  21 | loss  5.04 | perplexity   154.82| elapsed 44.96s \n",
            "epoch  22 | loss  4.96 | perplexity   142.83| elapsed 47.11s \n",
            "epoch  23 | loss  4.88 | perplexity   132.21| elapsed 49.25s \n",
            "epoch  24 | loss  4.81 | perplexity   123.09| elapsed 51.39s \n",
            "epoch  25 | loss  4.75 | perplexity   115.40| elapsed 53.54s \n",
            "epoch  26 | loss  4.70 | perplexity   109.82| elapsed 55.75s \n",
            "epoch  27 | loss  4.60 | perplexity    99.42| elapsed 57.94s \n",
            "epoch  28 | loss  4.51 | perplexity    91.26| elapsed 60.15s \n",
            "epoch  29 | loss  4.43 | perplexity    84.11| elapsed 62.29s \n",
            "epoch  30 | loss  4.36 | perplexity    78.06| elapsed 64.39s \n",
            "epoch  31 | loss  4.28 | perplexity    71.98| elapsed 66.53s \n",
            "epoch  32 | loss  4.20 | perplexity    66.85| elapsed 68.68s \n",
            "epoch  33 | loss  4.12 | perplexity    61.86| elapsed 70.79s \n",
            "epoch  34 | loss  4.05 | perplexity    57.58| elapsed 72.91s \n",
            "epoch  35 | loss  3.98 | perplexity    53.35| elapsed 75.06s \n",
            "epoch  36 | loss  3.90 | perplexity    49.55| elapsed 77.21s \n",
            "epoch  37 | loss  3.83 | perplexity    46.11| elapsed 79.33s \n",
            "epoch  38 | loss  3.75 | perplexity    42.73| elapsed 81.46s \n",
            "epoch  39 | loss  3.69 | perplexity    39.93| elapsed 83.61s \n",
            "epoch  40 | loss  3.62 | perplexity    37.27| elapsed 85.75s \n",
            "epoch  41 | loss  3.55 | perplexity    34.92| elapsed 87.91s \n",
            "epoch  42 | loss  3.50 | perplexity    33.15| elapsed 90.06s \n",
            "epoch  43 | loss  3.45 | perplexity    31.64| elapsed 92.21s \n",
            "epoch  44 | loss  3.38 | perplexity    29.31| elapsed 94.34s \n",
            "epoch  45 | loss  3.33 | perplexity    27.91| elapsed 96.49s \n",
            "epoch  46 | loss  3.28 | perplexity    26.46| elapsed 98.60s \n",
            "epoch  47 | loss  3.20 | perplexity    24.60| elapsed 100.71s \n",
            "epoch  48 | loss  3.12 | perplexity    22.71| elapsed 102.85s \n",
            "epoch  49 | loss  3.06 | perplexity    21.29| elapsed 104.95s \n",
            "epoch  50 | loss  2.98 | perplexity    19.77| elapsed 107.16s \n",
            "epoch  51 | loss  2.92 | perplexity    18.46| elapsed 109.30s \n",
            "epoch  52 | loss  2.85 | perplexity    17.24| elapsed 111.44s \n",
            "epoch  53 | loss  2.78 | perplexity    16.05| elapsed 113.59s \n",
            "epoch  54 | loss  2.71 | perplexity    15.02| elapsed 115.73s \n",
            "epoch  55 | loss  2.65 | perplexity    14.09| elapsed 117.88s \n",
            "epoch  56 | loss  2.59 | perplexity    13.31| elapsed 120.03s \n",
            "epoch  57 | loss  2.53 | perplexity    12.61| elapsed 122.17s \n",
            "epoch  58 | loss  2.48 | perplexity    11.98| elapsed 124.26s \n",
            "epoch  59 | loss  2.44 | perplexity    11.46| elapsed 126.38s \n",
            "epoch  60 | loss  2.38 | perplexity    10.79| elapsed 128.48s \n",
            "epoch  61 | loss  2.33 | perplexity    10.32| elapsed 130.61s \n",
            "epoch  62 | loss  2.29 | perplexity     9.84| elapsed 132.71s \n",
            "epoch  63 | loss  2.24 | perplexity     9.44| elapsed 134.85s \n",
            "epoch  64 | loss  2.21 | perplexity     9.11| elapsed 136.99s \n",
            "epoch  65 | loss  2.16 | perplexity     8.67| elapsed 139.13s \n",
            "epoch  66 | loss  2.12 | perplexity     8.35| elapsed 141.23s \n",
            "epoch  67 | loss  2.08 | perplexity     8.04| elapsed 143.37s \n",
            "epoch  68 | loss  2.04 | perplexity     7.72| elapsed 145.52s \n",
            "epoch  69 | loss  2.00 | perplexity     7.42| elapsed 147.67s \n",
            "epoch  70 | loss  1.97 | perplexity     7.18| elapsed 149.81s \n",
            "epoch  71 | loss  1.93 | perplexity     6.91| elapsed 151.94s \n",
            "epoch  72 | loss  1.90 | perplexity     6.67| elapsed 154.05s \n",
            "epoch  73 | loss  1.87 | perplexity     6.47| elapsed 156.19s \n",
            "epoch  74 | loss  1.84 | perplexity     6.30| elapsed 158.30s \n",
            "epoch  75 | loss  1.81 | perplexity     6.12| elapsed 160.43s \n",
            "epoch  76 | loss  1.77 | perplexity     5.86| elapsed 162.59s \n",
            "epoch  77 | loss  1.74 | perplexity     5.68| elapsed 164.73s \n",
            "epoch  78 | loss  1.71 | perplexity     5.52| elapsed 166.87s \n",
            "epoch  79 | loss  1.68 | perplexity     5.35| elapsed 169.01s \n",
            "epoch  80 | loss  1.64 | perplexity     5.18| elapsed 171.16s \n",
            "epoch  81 | loss  1.62 | perplexity     5.05| elapsed 173.30s \n",
            "epoch  82 | loss  1.59 | perplexity     4.92| elapsed 175.45s \n",
            "epoch  83 | loss  1.57 | perplexity     4.79| elapsed 177.55s \n",
            "epoch  84 | loss  1.55 | perplexity     4.69| elapsed 179.68s \n",
            "epoch  85 | loss  1.52 | perplexity     4.57| elapsed 181.80s \n",
            "epoch  86 | loss  1.49 | perplexity     4.44| elapsed 183.92s \n",
            "epoch  87 | loss  1.46 | perplexity     4.32| elapsed 186.02s \n",
            "epoch  88 | loss  1.44 | perplexity     4.24| elapsed 188.16s \n",
            "epoch  89 | loss  1.42 | perplexity     4.12| elapsed 190.36s \n",
            "epoch  90 | loss  1.40 | perplexity     4.04| elapsed 192.51s \n",
            "epoch  91 | loss  1.37 | perplexity     3.94| elapsed 194.66s \n",
            "epoch  92 | loss  1.35 | perplexity     3.86| elapsed 196.79s \n",
            "epoch  93 | loss  1.33 | perplexity     3.76| elapsed 198.92s \n",
            "epoch  94 | loss  1.31 | perplexity     3.70| elapsed 201.03s \n",
            "epoch  95 | loss  1.30 | perplexity     3.65| elapsed 203.18s \n",
            "epoch  96 | loss  1.27 | perplexity     3.57| elapsed 205.33s \n",
            "epoch  97 | loss  1.26 | perplexity     3.52| elapsed 207.48s \n",
            "epoch  98 | loss  1.24 | perplexity     3.45| elapsed 209.62s \n",
            "epoch  99 | loss  1.22 | perplexity     3.40| elapsed 211.77s \n",
            "epoch 100 | loss  1.20 | perplexity     3.33| elapsed 213.90s \n",
            "Zipped files at /content/data/karinthy-wp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oueW91h7bjak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate verses! "
      ]
    },
    {
      "metadata": {
        "id": "vtS63EkMOIbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "import torch\n",
        "\n",
        "def is_unbalanced(s):\n",
        "  if s.count('\"') % 2 != 0 or s.count('(') != s.count(')'):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def sample_punkt():\n",
        "  return sample(['.', '?', '!'], 1)[0]\n",
        "\n",
        "def parse_last_line(s):\n",
        "  l = list(s)\n",
        "  \n",
        "  if l[-1] == ',': \n",
        "    l[-1] = sample_punkt()\n",
        "  \n",
        "  if l[-1] not in list('.?!'): \n",
        "    l.append(sample_punkt())  \n",
        "  return \"\".join(l)\n",
        "  \n",
        "def generate_line(model, hidden=None, temp=1.0, \n",
        "               sos_id=1, eos_id=2, unk_id=0, max_len=None):\n",
        "  \"\"\"Generate line from `model` with `hidden` state at `temp`.\"\"\"\n",
        "  ids = []\n",
        "  \n",
        "  if hidden is None:\n",
        "    hidden = model.init_hidden(1)\n",
        "  \n",
        "  input = torch.tensor([sos_id], dtype=torch.long).reshape(1,1).to(device)\n",
        "  \n",
        "  id = 0\n",
        "  while id != eos_id and len(ids)<max_len :\n",
        "    output, hidden = model(input, hidden)\n",
        "    probs = output.squeeze().div(temp).exp().cpu() \n",
        "    id = torch.multinomial(probs, num_samples=1).item() \n",
        "    if id == sos_id or id == unk_id: continue\n",
        "    input.fill_(id)\n",
        "    ids += [id]\n",
        "  \n",
        "  return ids, hidden\n",
        "\n",
        "def generate(model, tokenizer, num_lines=8, min_len=8, max_len=15,\n",
        "             unk_id=0, sos_id=1, eos_id=2, temp=0.6):\n",
        "  \"\"\" \n",
        "  Generate a verse consisting of `num_lines` lines of max. length `max_tokens`.\n",
        "  Since the hidden state is passed onto the next line, \n",
        "  observing some cross-line consistency would be expected, or less\n",
        "  optimistically, at least grammatically correct sentences.\n",
        "  NOTE: line length can be tuned by changing max_tokens (i.e. subword pieces).\n",
        "  \n",
        "  Args:\n",
        "    model: Trained PyTorch language model\n",
        "    tokenizer: SentencePiece tokenizer\n",
        "    temp: Temperature parameter; lower: more conservative, higher: more diverse\n",
        "    num_lines: No. of lines to generate.\n",
        "    max_len: Max no. of tokens per line (not words!)\n",
        "    sos_id: Start of sequence id in vocabulary\n",
        "    eos_id: End of sequence id in vocabulary\n",
        "  \n",
        "  Returns: list of strings\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  \n",
        "  lines = []\n",
        "  line_cnt = 0\n",
        "  hidden = model.init_hidden(1)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    while line_cnt != num_lines:\n",
        "      try:\n",
        "\n",
        "        ids, hidden = generate_line(model, hidden=hidden, temp=temp, max_len=max_len,\n",
        "                                    sos_id=sos_id, eos_id=eos_id, unk_id=unk_id)\n",
        "        \n",
        "        if len(ids) <= min_len: raise Exception\n",
        "        line = tokenizer.textify(ids).strip()\n",
        "        \n",
        "        if line.startswith(tuple(\"-?!.,()\")): raise Exception\n",
        "        if is_unbalanced(line): raise Exception\n",
        "        \n",
        "        lines += [line]\n",
        "        line_cnt +=1\n",
        "        \n",
        "      except Exception as e:\n",
        "        pass\n",
        "    \n",
        "  last_line = lines.pop()\n",
        "  l = parse_last_line(last_line)\n",
        "  lines.append(l)\n",
        "  \n",
        "  return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73w_4CMz-Mrv",
        "colab_type": "code",
        "outputId": "73286ce4-a6e8-466f-bedd-13628dea4e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.3, num_lines=14, min_len=6, max_len=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Helyettük mérkőzünk uj Curiatiusok',\n",
              " 'Ha hozzájutómonban és Busch modorában',\n",
              " 'Hollósy mesternek a hóhér,',\n",
              " 'Pörögnek a dobverők.',\n",
              " 'ux Armes Citoyens! Aux Armes...!',\n",
              " 'ösem, a hírt, a szemtelen bolond szél',\n",
              " 'hírt, aki csinálja, mintha',\n",
              " 'hírt, tajtékos ajkán sápadt mosoly,',\n",
              " 'hírt, Jós a hajó,',\n",
              " 'Mea, te régi, fáradt rézkapocs,',\n",
              " 'Mea, te is, handlé,',\n",
              " 'hírt, meghal, ki, pálya, pálya, pálya.',\n",
              " 'Ha hozzáje, hogy bztalap,',\n",
              " 'cifra zsitölnek!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "9mCMMjXJQOQ-",
        "colab_type": "code",
        "outputId": "959299b5-8e67-4340-d678-ba0d54f4f657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.6, num_lines=14, min_len=6, max_len=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Könnyes, fakó szívemmel, gyalázatos kor, köszönöm',\n",
              " 'Ha mindent felelnek, ha nem önt szerette?',\n",
              " 'ehet! A miután a másik:',\n",
              " 'barátja, a báncgyen',\n",
              " 'Miért nem rántsz lefelé,',\n",
              " 'Aztán? Desmoulins kiált!',\n",
              " 'Pötal-ragadtan.',\n",
              " 'Ha még miért nem értik nálatok',\n",
              " 'inek? meghalt, mint a habok',\n",
              " 'Ki! tele van bennerem én',\n",
              " 'Puhabigyerek,',\n",
              " 'S hajag és Erő és zörög,',\n",
              " 'Aróra, ittak, ettek,',\n",
              " 'Halljátok? Desmoulins kiált!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "_yNUM_HPSpvw",
        "colab_type": "code",
        "outputId": "fefcc89d-8b4b-4233-c844-54db731968e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.7, num_lines=10, max_len=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bszolut, mint a kutyának.',\n",
              " 'molett és torlódó tenger',\n",
              " 'Telmed, de igazán s az istenért',\n",
              " 'Tagarkba rohan tovább a könny',\n",
              " 'halagosan és torlódik tőle minden atom',\n",
              " '\"Eet\" és \"nem szeret\" sokezer akácfa leveléből',\n",
              " 'szén betölteni a test idegbe',\n",
              " 'megbomra ez a görcsös izom',\n",
              " 'szopiker Gon\"\" és \"nem\"',\n",
              " 'mi igaz és mi a hazugság.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    }
  ]
}