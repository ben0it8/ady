{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arany-language-model-wordpiece.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben0it8/poetry-language-model/blob/master/arany_language_model_wordpiece.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3Syp72_eY06e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Install extra requirements"
      ]
    },
    {
      "metadata": {
        "id": "EAGNkWy_D5er",
        "colab_type": "code",
        "outputId": "993c4167-de56-4c30-a896-df6b3154b3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U ftfy\n",
        "!pip install -U sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.1.7)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.5.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/8a/0e4a10bc00a0263db8d45d0062c83892598eb58e8091f439c63926e9b107/sentencepiece-0.1.81-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 19.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYlrr4nDEk5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'; # adapt plots for retina displays\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid');\n",
        "sns.set_context(context='notebook');\n",
        "from typing import List\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import numpy as np \n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import ftfy \n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import dill\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, tensor\n",
        "from torch.nn import functional as F\n",
        "from random import sample\n",
        "\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "data_dir = Path(\"data/arany\").resolve()\n",
        "data_dir.mkdir(exist_ok=True, parents=True)\n",
        "url = \"https://raw.githubusercontent.com/ben0it8/ady/master/data/arany.txt\" \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bIaijzWl2G-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_paths(path):\n",
        "  path = Path(path)\n",
        "  ret = []\n",
        "  for x in path.glob('*'):\n",
        "    if x.is_file() and not str(x).endswith('.zip'):\n",
        "      ret.append(str(x))\n",
        "  return ret\n",
        "\n",
        "def zip_dir(path, name):\n",
        "  if not name.endswith('.zip'): name += '.zip'\n",
        "  file_paths = get_file_paths(path)\n",
        "  \n",
        "  with ZipFile(path/name, 'w') as zip:\n",
        "    for file in file_paths:\n",
        "      zip.write(file, os.path.basename(file))\n",
        "  print(f\"Zipped files at {path}\")\n",
        "  return (path/name).resolve()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIxEBl5Irzky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Read data"
      ]
    },
    {
      "metadata": {
        "id": "-HobfQaar-ID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean data & write to disk"
      ]
    },
    {
      "metadata": {
        "id": "LidRmCTeFDP-",
        "colab_type": "code",
        "outputId": "41c12b7d-1f4c-44be-c4f8-99e4621175ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "def is_title(t):\n",
        "  t = re.sub('[\\d\\s\\s\\-!?,]', '', t)\n",
        "  return t.isupper()\n",
        "\n",
        "def fix_text(t:str):\n",
        "  t = ftfy.fix_text(t, normalization='NFKC')\n",
        "  t = t.replace('\\n', '') # remove newlines\n",
        "  t = re.sub(r'[»«]', '', t) # remove special parenthesis\n",
        "  t = re.sub(r'[0-9]','', t)\n",
        "  t = re.sub(\"\\s\\s+\", \" \", t) # skip whitespaces\n",
        "  t = re.sub(\"A.J.\", \"\", t)\n",
        "  t = re.sub(\"A. J.\", \"\", t)\n",
        "  t = t.strip()\n",
        "  return t\n",
        "    \n",
        "def fix_texts(texts:list):\n",
        "  out = []\n",
        "  for i, line in enumerate(texts):\n",
        "    if \"\\u2424\" in line:\n",
        "      line = line.split(\"\\u2424\")\n",
        "    elif '\\u2028' in line:\n",
        "      line = line.split('\\u2028')\n",
        "    elif '\\u000A' in line:\n",
        "      line = line.split('\\u000A')\n",
        "    else:\n",
        "      line = [line] \n",
        "    for t in line:\n",
        "      t = fix_text(t)\n",
        "      if (t is None or len(t.replace(' ', ''))<=3 or is_title(t) or\n",
        "          t.startswith(('.', ',', '?', '!', '-', ';')) or len(t) > 100):\n",
        "        continue\n",
        "      else:\n",
        "        out += [t]   \n",
        "  return out\n",
        "\n",
        "response =  requests.get(url)\n",
        "texts = [line.decode() for line in response.iter_lines()]\n",
        "print(f\"No. of lines: {len(texts)}\")\n",
        "clean_texts = fix_texts(texts)\n",
        "(data_dir/'text_clean.txt').open(mode='wt').writelines(f\"{line}\\n\" for line in clean_texts)\n",
        "print(f\"No. of lines: {len(clean_texts)}\")\n",
        "np.random.shuffle(clean_texts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of lines: 57384\n",
            "No. of lines: 49495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-m1fCHxGySxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Tokenizer (which uses SentencePiece), Corpus ( data handler)"
      ]
    },
    {
      "metadata": {
        "id": "wGJipK5GmgsT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    \n",
        "    ID_UNK = 0\n",
        "    ID_SOS = 1\n",
        "    ID_EOS = 2\n",
        "    \n",
        "    def __init__(self, model_path:str):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.Load(str(model_path))\n",
        "        self.id_unk = self.sp.unk_id()\n",
        "        self.tk_unk = self.sp.IdToPiece(self.id_unk)\n",
        "        self.tk_sos = self.sp.IdToPiece(self.ID_SOS)\n",
        "        self.tk_eos = self.sp.IdToPiece(self.ID_EOS)\n",
        "        self.vocab_size = len(self.sp)\n",
        "        logger.info(f\"Initialized SentPieceProcessor from {model_path}\")\n",
        "    \n",
        "    def numericalize(self, tokens: List[str]) -> List[List[int]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        ids =  [self.sp.EncodeAsIds(s) for s in tokens]\n",
        "        if len(ids) == 1: ids=ids[0]\n",
        "        return ids\n",
        "\n",
        "    def piecify(self, tokens: List[str]) -> List[List[str]]:\n",
        "        if isinstance(tokens, str): tokens = [tokens]\n",
        "        return [self.sp.EncodeAsPieces(s) for s in tokens]\n",
        "    \n",
        "    def textify(self, ids: List[int]) -> str:\n",
        "        if isinstance(ids, list) and isinstance(ids[0], np.generic): \n",
        "            ids = [int(x) for x in ids]\n",
        "        if not isinstance(ids, list) and not isinstance(ids[0], int):\n",
        "            raise TypeError(\"Argument `ids` has to be a list of integers.\")            \n",
        "        return self.sp.DecodeIds(ids)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_file(cls, input_file:str, output_path:str='default', vocab_size:int=16000, \n",
        "                  char_cov:float=1.0, model_type:str='unigram'):\n",
        "        \n",
        "        assert model_type in ['unigram', 'bpe', 'char', 'word']\n",
        "        assert 0 < char_cov <= 1\n",
        "        input_file = str(input_file)\n",
        "        output_file =  os.path.splitext(str(output_path))[0]\n",
        "        ext = '.model'\n",
        "        train_cmd = f\"--input={input_file} --model_prefix={output_file}\"\\\n",
        "                    f\" --vocab_size={vocab_size} --character_coverage={char_cov} --model_type={model_type}\"\n",
        "\n",
        "        logger.info(f\"Train command: {train_cmd}\")\n",
        "        logger.info(f\"Started training SentencePiece model...\")\n",
        "        ret = spm.SentencePieceTrainer.Train(train_cmd)\n",
        "        logger.info(f\"Exit code: {int(ret)}\")\n",
        "        return cls(output_file+ext)\n",
        "      \n",
        "def batchify(data, bsz):\n",
        "    # work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "class Corpus(object):\n",
        "\n",
        "  def __init__(self, file_path, tokenizer, bs=20):\n",
        "       \n",
        "        self.processor = tokenizer\n",
        "        self.id_sos, self.id_eos = self.processor.ID_SOS, self.processor.ID_EOS\n",
        "        self.bs = bs\n",
        "        self.data = self.tokenize(file_path)        \n",
        "        self.vocab_size = self.processor.vocab_size\n",
        "        \n",
        "  def tokenize(self, path):\n",
        "      \"\"\"Tokenizes a text file.\"\"\"\n",
        "      ids = []\n",
        "      with open(path, 'r') as f:\n",
        "          for line in f:\n",
        "              numericalized = self.processor.numericalize(line)\n",
        "              ids.extend([self.id_sos] + numericalized + [self.id_eos])\n",
        "\n",
        "      return batchify(torch.LongTensor(ids), self.bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTn7LTQfkJlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define RNN model architecture,  training loop and helpers"
      ]
    },
    {
      "metadata": {
        "id": "bi951Z9uzzCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, emsize, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, emsize)\n",
        "        assert (rnn_type in ['LSTM', 'GRU']), \"Arg `rnn_type` has to be one of {GRU, LSTM}.\"\n",
        "        self.rnn = getattr(nn, rnn_type)(emsize, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        if tie_weights:\n",
        "            if nhid != emsize:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "       \n",
        "      \n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "  if isinstance(h, torch.Tensor):\n",
        "    return h.detach()\n",
        "  else:\n",
        "    return tuple(repackage_hidden(v) for v in h)\n",
        "      \n",
        "def get_num_params(model):\n",
        "  return sum([p.nelement() for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "def get_batch(source, i, bptt):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "  \n",
        "def train_epoch(train_data, model, vocab_size, bs=16, bptt=20, clip=.25):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    hidden = model.init_hidden(bs)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        total_loss += len(data) * loss.item()\n",
        "        \n",
        "    return total_loss / (len(train_data) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w62PRkgBJS7W",
        "colab_type": "code",
        "outputId": "7b92ed24-fb05-4889-b435-6c8b1a0a3a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "# INIT AND TRAIN TOKENIZER FROM CLEAN TEXTS\n",
        "bs = 64\n",
        "\n",
        "model_type = 'unigram' # can be \"bpe\"/\"unigram\" to support wordpieces\n",
        "\n",
        "vocab_size = 20000 # how many wordpieces to consider\n",
        "\n",
        "tokenizer = Tokenizer.from_file(data_dir/'text_clean.txt', \n",
        "                                output_path=data_dir/'tokenizer', char_cov=1.0,\n",
        "                                model_type=model_type, vocab_size=vocab_size) \n",
        "\n",
        "# INIT CORPUS FROM CLEAN TEXTS AND TOKENIZER WITH BATCH_SIZE `BS`\n",
        "\n",
        "corpus = Corpus(data_dir/'text_clean.txt', tokenizer=tokenizer, bs=bs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-10 13:50:13,550 : INFO : Train command: --input=/content/data/arany/text_clean.txt --model_prefix=/content/data/arany/tokenizer --vocab_size=20000 --character_coverage=1.0 --model_type=unigram\n",
            "2019-04-10 13:50:13,552 : INFO : Started training SentencePiece model...\n",
            "2019-04-10 13:50:22,044 : INFO : Exit code: 1\n",
            "2019-04-10 13:50:22,085 : INFO : Initialized SentPieceProcessor from /content/data/arany/tokenizer.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3BDcrSMzjNP",
        "colab_type": "code",
        "outputId": "18818997-c8bd-446d-bb06-8010454ef682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "# LSTM PARAMETERS\n",
        "model_type='GRU' # less params than LSTM\n",
        "\n",
        "emsize = 400\n",
        "nhid = 400\n",
        "\n",
        "nlayers = 1\n",
        "\n",
        "dropout = 0.1 # > 0.2 gave hard time converging\n",
        "clip = 3.0 # lower seemmed to make convergence difficult\n",
        "\n",
        "tied = True # ideal for word/wordpiece level\n",
        "\n",
        "bptt = 80 # the higher the more context info; compute trade-off\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "# INIT LSTM MODEL, LOSS FUNCTION AND OPTIMIZER\n",
        "\n",
        "model = RNNModel(model_type, corpus.vocab_size, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "print(f\"No. of parameters: {get_num_params(model)}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "No. of parameters: 8982400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FXRCWjOZaBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's train the model"
      ]
    },
    {
      "metadata": {
        "id": "H3zNzPST0eYI",
        "colab_type": "code",
        "outputId": "307b74e0-8ae9-47ae-fada-b97bebdbd0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2584
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 150\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "  \n",
        "  for epoch in range(1, epochs+1):\n",
        "\n",
        "    loss = train_epoch(corpus.data, model, corpus.vocab_size, \n",
        "                       clip=clip, bs=bs, bptt=bptt)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"epoch {:3d} | loss {:5.2f} | perplexity {:8.2f}| elapsed {:5.2f}s \".format(epoch, loss, math.exp(loss), elapsed))\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print('-' * 89)\n",
        "  print('Exiting from training early')\n",
        "\n",
        "finally:\n",
        "  params = {\"model_type\": model_type,\n",
        "            \"ntoken\": corpus.vocab_size,\n",
        "            \"emsize\": emsize,\n",
        "            \"nhid\": nhid,\n",
        "            \"nlayers\": nlayers,\n",
        "            \"dropout\": dropout,\n",
        "            \"tied\": tied}\n",
        " \n",
        "  with open(data_dir/'model_state.pth', 'wb') as f:\n",
        "    torch.save({\"state_dict\": model.state_dict(),\n",
        "                \"params\": params}, f)\n",
        "    \n",
        "zipfile = zip_dir(data_dir, data_dir.name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch   1 | loss  6.60 | perplexity   737.11| elapsed 20.99s \n",
            "epoch   2 | loss  5.87 | perplexity   355.47| elapsed 41.88s \n",
            "epoch   3 | loss  5.65 | perplexity   285.71| elapsed 62.77s \n",
            "epoch   4 | loss  5.48 | perplexity   239.01| elapsed 83.69s \n",
            "epoch   5 | loss  5.32 | perplexity   204.84| elapsed 104.66s \n",
            "epoch   6 | loss  5.18 | perplexity   177.63| elapsed 125.68s \n",
            "epoch   7 | loss  5.04 | perplexity   153.96| elapsed 146.68s \n",
            "epoch   8 | loss  4.89 | perplexity   133.44| elapsed 167.64s \n",
            "epoch   9 | loss  4.75 | perplexity   115.37| elapsed 188.59s \n",
            "epoch  10 | loss  4.61 | perplexity   100.31| elapsed 209.62s \n",
            "epoch  11 | loss  4.47 | perplexity    87.46| elapsed 230.65s \n",
            "epoch  12 | loss  4.33 | perplexity    76.19| elapsed 251.71s \n",
            "epoch  13 | loss  4.20 | perplexity    66.61| elapsed 272.68s \n",
            "epoch  14 | loss  4.07 | perplexity    58.63| elapsed 293.68s \n",
            "epoch  15 | loss  3.95 | perplexity    51.78| elapsed 314.71s \n",
            "epoch  16 | loss  3.83 | perplexity    45.97| elapsed 335.77s \n",
            "epoch  17 | loss  3.71 | perplexity    40.94| elapsed 356.85s \n",
            "epoch  18 | loss  3.59 | perplexity    36.28| elapsed 377.83s \n",
            "epoch  19 | loss  3.48 | perplexity    32.33| elapsed 398.84s \n",
            "epoch  20 | loss  3.36 | perplexity    28.93| elapsed 419.86s \n",
            "epoch  21 | loss  3.26 | perplexity    26.17| elapsed 440.91s \n",
            "epoch  22 | loss  3.17 | perplexity    23.79| elapsed 461.93s \n",
            "epoch  23 | loss  3.09 | perplexity    21.88| elapsed 482.92s \n",
            "epoch  24 | loss  3.00 | perplexity    20.08| elapsed 503.90s \n",
            "epoch  25 | loss  2.92 | perplexity    18.46| elapsed 524.93s \n",
            "epoch  26 | loss  2.83 | perplexity    17.02| elapsed 545.97s \n",
            "epoch  27 | loss  2.76 | perplexity    15.79| elapsed 567.04s \n",
            "epoch  28 | loss  2.69 | perplexity    14.77| elapsed 587.99s \n",
            "epoch  29 | loss  2.63 | perplexity    13.82| elapsed 608.99s \n",
            "epoch  30 | loss  2.56 | perplexity    12.98| elapsed 629.98s \n",
            "epoch  31 | loss  2.50 | perplexity    12.23| elapsed 651.01s \n",
            "epoch  32 | loss  2.45 | perplexity    11.59| elapsed 672.05s \n",
            "epoch  33 | loss  2.40 | perplexity    11.02| elapsed 692.99s \n",
            "epoch  34 | loss  2.35 | perplexity    10.54| elapsed 713.95s \n",
            "epoch  35 | loss  2.31 | perplexity    10.09| elapsed 734.92s \n",
            "epoch  36 | loss  2.27 | perplexity     9.71| elapsed 755.94s \n",
            "epoch  37 | loss  2.24 | perplexity     9.35| elapsed 776.95s \n",
            "epoch  38 | loss  2.20 | perplexity     9.05| elapsed 797.87s \n",
            "epoch  39 | loss  2.17 | perplexity     8.72| elapsed 818.84s \n",
            "epoch  40 | loss  2.12 | perplexity     8.34| elapsed 839.84s \n",
            "epoch  41 | loss  2.08 | perplexity     8.02| elapsed 860.85s \n",
            "epoch  42 | loss  2.05 | perplexity     7.74| elapsed 881.84s \n",
            "epoch  43 | loss  2.01 | perplexity     7.45| elapsed 902.78s \n",
            "epoch  44 | loss  1.98 | perplexity     7.21| elapsed 923.75s \n",
            "epoch  45 | loss  1.95 | perplexity     7.01| elapsed 944.70s \n",
            "epoch  46 | loss  1.92 | perplexity     6.81| elapsed 965.69s \n",
            "epoch  47 | loss  1.89 | perplexity     6.65| elapsed 986.70s \n",
            "epoch  48 | loss  1.87 | perplexity     6.48| elapsed 1007.65s \n",
            "epoch  49 | loss  1.84 | perplexity     6.32| elapsed 1028.61s \n",
            "epoch  50 | loss  1.82 | perplexity     6.18| elapsed 1049.56s \n",
            "epoch  51 | loss  1.80 | perplexity     6.05| elapsed 1070.54s \n",
            "epoch  52 | loss  1.78 | perplexity     5.94| elapsed 1091.52s \n",
            "epoch  53 | loss  1.76 | perplexity     5.83| elapsed 1112.43s \n",
            "epoch  54 | loss  1.74 | perplexity     5.71| elapsed 1133.37s \n",
            "epoch  55 | loss  1.72 | perplexity     5.59| elapsed 1154.36s \n",
            "epoch  56 | loss  1.70 | perplexity     5.48| elapsed 1175.39s \n",
            "epoch  57 | loss  1.68 | perplexity     5.39| elapsed 1196.40s \n",
            "epoch  58 | loss  1.67 | perplexity     5.29| elapsed 1217.36s \n",
            "epoch  59 | loss  1.65 | perplexity     5.21| elapsed 1238.30s \n",
            "epoch  60 | loss  1.63 | perplexity     5.12| elapsed 1259.27s \n",
            "epoch  61 | loss  1.61 | perplexity     5.03| elapsed 1280.27s \n",
            "epoch  62 | loss  1.60 | perplexity     4.96| elapsed 1301.27s \n",
            "epoch  63 | loss  1.59 | perplexity     4.89| elapsed 1322.21s \n",
            "epoch  64 | loss  1.57 | perplexity     4.82| elapsed 1343.15s \n",
            "epoch  65 | loss  1.56 | perplexity     4.75| elapsed 1364.11s \n",
            "epoch  66 | loss  1.54 | perplexity     4.69| elapsed 1385.10s \n",
            "epoch  67 | loss  1.53 | perplexity     4.62| elapsed 1406.07s \n",
            "epoch  68 | loss  1.52 | perplexity     4.57| elapsed 1426.99s \n",
            "epoch  69 | loss  1.51 | perplexity     4.52| elapsed 1447.92s \n",
            "epoch  70 | loss  1.50 | perplexity     4.47| elapsed 1468.87s \n",
            "epoch  71 | loss  1.48 | perplexity     4.41| elapsed 1489.83s \n",
            "epoch  72 | loss  1.47 | perplexity     4.37| elapsed 1510.83s \n",
            "epoch  73 | loss  1.46 | perplexity     4.32| elapsed 1531.72s \n",
            "epoch  74 | loss  1.45 | perplexity     4.27| elapsed 1552.67s \n",
            "epoch  75 | loss  1.44 | perplexity     4.23| elapsed 1573.63s \n",
            "epoch  76 | loss  1.43 | perplexity     4.17| elapsed 1594.59s \n",
            "epoch  77 | loss  1.42 | perplexity     4.14| elapsed 1615.57s \n",
            "epoch  78 | loss  1.41 | perplexity     4.10| elapsed 1636.46s \n",
            "epoch  79 | loss  1.40 | perplexity     4.06| elapsed 1657.38s \n",
            "epoch  80 | loss  1.39 | perplexity     4.02| elapsed 1678.31s \n",
            "epoch  81 | loss  1.38 | perplexity     3.98| elapsed 1699.28s \n",
            "epoch  82 | loss  1.37 | perplexity     3.95| elapsed 1720.25s \n",
            "epoch  83 | loss  1.36 | perplexity     3.91| elapsed 1741.12s \n",
            "epoch  84 | loss  1.36 | perplexity     3.88| elapsed 1762.03s \n",
            "epoch  85 | loss  1.35 | perplexity     3.85| elapsed 1782.98s \n",
            "epoch  86 | loss  1.34 | perplexity     3.82| elapsed 1803.99s \n",
            "epoch  87 | loss  1.33 | perplexity     3.78| elapsed 1824.97s \n",
            "epoch  88 | loss  1.32 | perplexity     3.75| elapsed 1845.83s \n",
            "epoch  89 | loss  1.32 | perplexity     3.73| elapsed 1866.76s \n",
            "epoch  90 | loss  1.30 | perplexity     3.68| elapsed 1887.68s \n",
            "epoch  91 | loss  1.30 | perplexity     3.66| elapsed 1908.64s \n",
            "epoch  92 | loss  1.29 | perplexity     3.64| elapsed 1929.62s \n",
            "epoch  93 | loss  1.28 | perplexity     3.61| elapsed 1950.52s \n",
            "epoch  94 | loss  1.28 | perplexity     3.58| elapsed 1971.43s \n",
            "epoch  95 | loss  1.27 | perplexity     3.56| elapsed 1992.36s \n",
            "epoch  96 | loss  1.26 | perplexity     3.53| elapsed 2013.39s \n",
            "epoch  97 | loss  1.25 | perplexity     3.51| elapsed 2034.39s \n",
            "epoch  98 | loss  1.25 | perplexity     3.48| elapsed 2055.33s \n",
            "epoch  99 | loss  1.24 | perplexity     3.46| elapsed 2076.29s \n",
            "epoch 100 | loss  1.24 | perplexity     3.45| elapsed 2097.28s \n",
            "epoch 101 | loss  1.23 | perplexity     3.43| elapsed 2118.23s \n",
            "epoch 102 | loss  1.22 | perplexity     3.39| elapsed 2139.21s \n",
            "epoch 103 | loss  1.22 | perplexity     3.38| elapsed 2160.09s \n",
            "epoch 104 | loss  1.21 | perplexity     3.35| elapsed 2181.01s \n",
            "epoch 105 | loss  1.20 | perplexity     3.32| elapsed 2201.95s \n",
            "epoch 106 | loss  1.20 | perplexity     3.31| elapsed 2222.88s \n",
            "epoch 107 | loss  1.19 | perplexity     3.29| elapsed 2243.82s \n",
            "epoch 108 | loss  1.18 | perplexity     3.27| elapsed 2264.71s \n",
            "epoch 109 | loss  1.18 | perplexity     3.25| elapsed 2285.62s \n",
            "epoch 110 | loss  1.17 | perplexity     3.24| elapsed 2306.54s \n",
            "epoch 111 | loss  1.17 | perplexity     3.22| elapsed 2327.51s \n",
            "epoch 112 | loss  1.16 | perplexity     3.20| elapsed 2348.44s \n",
            "epoch 113 | loss  1.16 | perplexity     3.18| elapsed 2369.32s \n",
            "epoch 114 | loss  1.15 | perplexity     3.16| elapsed 2390.21s \n",
            "epoch 115 | loss  1.14 | perplexity     3.14| elapsed 2411.13s \n",
            "epoch 116 | loss  1.14 | perplexity     3.12| elapsed 2432.08s \n",
            "epoch 117 | loss  1.13 | perplexity     3.10| elapsed 2452.98s \n",
            "epoch 118 | loss  1.13 | perplexity     3.10| elapsed 2473.87s \n",
            "epoch 119 | loss  1.12 | perplexity     3.07| elapsed 2494.75s \n",
            "epoch 120 | loss  1.12 | perplexity     3.05| elapsed 2515.68s \n",
            "epoch 121 | loss  1.11 | perplexity     3.04| elapsed 2536.63s \n",
            "epoch 122 | loss  1.11 | perplexity     3.03| elapsed 2557.58s \n",
            "epoch 123 | loss  1.10 | perplexity     3.00| elapsed 2578.43s \n",
            "epoch 124 | loss  1.09 | perplexity     2.99| elapsed 2599.30s \n",
            "epoch 125 | loss  1.09 | perplexity     2.97| elapsed 2620.23s \n",
            "epoch 126 | loss  1.08 | perplexity     2.95| elapsed 2641.19s \n",
            "epoch 127 | loss  1.08 | perplexity     2.95| elapsed 2662.14s \n",
            "epoch 128 | loss  1.08 | perplexity     2.93| elapsed 2683.03s \n",
            "epoch 129 | loss  1.07 | perplexity     2.92| elapsed 2703.90s \n",
            "epoch 130 | loss  1.07 | perplexity     2.91| elapsed 2724.79s \n",
            "epoch 131 | loss  1.06 | perplexity     2.89| elapsed 2745.74s \n",
            "epoch 132 | loss  1.06 | perplexity     2.88| elapsed 2766.68s \n",
            "epoch 133 | loss  1.05 | perplexity     2.86| elapsed 2787.54s \n",
            "epoch 134 | loss  1.05 | perplexity     2.86| elapsed 2808.42s \n",
            "epoch 135 | loss  1.05 | perplexity     2.85| elapsed 2829.31s \n",
            "epoch 136 | loss  1.04 | perplexity     2.84| elapsed 2850.25s \n",
            "epoch 137 | loss  1.04 | perplexity     2.82| elapsed 2871.20s \n",
            "epoch 138 | loss  1.03 | perplexity     2.81| elapsed 2892.06s \n",
            "epoch 139 | loss  1.03 | perplexity     2.79| elapsed 2912.96s \n",
            "epoch 140 | loss  1.02 | perplexity     2.79| elapsed 2933.87s \n",
            "epoch 141 | loss  1.02 | perplexity     2.77| elapsed 2954.81s \n",
            "epoch 142 | loss  1.02 | perplexity     2.76| elapsed 2975.77s \n",
            "epoch 143 | loss  1.01 | perplexity     2.76| elapsed 2996.64s \n",
            "epoch 144 | loss  1.01 | perplexity     2.74| elapsed 3017.52s \n",
            "epoch 145 | loss  1.00 | perplexity     2.72| elapsed 3038.41s \n",
            "epoch 146 | loss  1.00 | perplexity     2.72| elapsed 3059.37s \n",
            "epoch 147 | loss  1.00 | perplexity     2.71| elapsed 3080.30s \n",
            "epoch 148 | loss  0.99 | perplexity     2.69| elapsed 3101.14s \n",
            "epoch 149 | loss  0.99 | perplexity     2.69| elapsed 3122.00s \n",
            "epoch 150 | loss  0.98 | perplexity     2.68| elapsed 3142.91s \n",
            "Zipped files at /content/data/arany\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oueW91h7bjak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate verses! "
      ]
    },
    {
      "metadata": {
        "id": "vtS63EkMOIbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "import torch\n",
        "\n",
        "def is_unbalanced(s):\n",
        "  if s.count('\"') % 2 != 0 or s.count('(') != s.count(')'):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def sample_punkt():\n",
        "  return sample(['.', '?', '!'], 1)[0]\n",
        "\n",
        "def parse_last_line(s):\n",
        "  l = list(s)\n",
        "  if l[-1] == ',': \n",
        "    l[-1] = sample_punkt()\n",
        "  \n",
        "  if l[-1] not in list('.?!'): \n",
        "    l.append(sample_punkt())  \n",
        "  return \"\".join(l)\n",
        "  \n",
        "def generate_line(model, hidden=None, temp=1.0, \n",
        "               sos_id=1, eos_id=2, unk_id=0, max_len=None):\n",
        "  \"\"\"Generate line from `model` with `hidden` state at `temp`.\"\"\"\n",
        "  ids = []\n",
        "  \n",
        "  if hidden is None:\n",
        "    hidden = model.init_hidden(1)\n",
        "  \n",
        "  input = torch.tensor([sos_id], dtype=torch.long).reshape(1,1).to(device)\n",
        "  \n",
        "  id = 0\n",
        "  while id != eos_id and len(ids)<max_len :\n",
        "    output, hidden = model(input, hidden)\n",
        "    probs = output.squeeze().div(temp).exp().cpu() \n",
        "    id = torch.multinomial(probs, num_samples=1).item() \n",
        "    if id == sos_id or id == unk_id: continue\n",
        "    input.fill_(id)\n",
        "    ids += [id]\n",
        "  \n",
        "  return ids, hidden\n",
        "\n",
        "def generate(model, tokenizer, num_lines=8, min_len=8, max_len=15,\n",
        "             unk_id=0, sos_id=1, eos_id=2, temp=0.6):\n",
        "  \"\"\" \n",
        "  Generate a verse consisting of `num_lines` lines of max. length `max_tokens`.\n",
        "  Since the hidden state is passed onto the next line, \n",
        "  observing some cross-line consistency would be expected, or less\n",
        "  optimistically, at least grammatically correct sentences.\n",
        "  NOTE: line length can be tuned by changing max_tokens (i.e. subword pieces).\n",
        "  \n",
        "  Args:\n",
        "    model: Trained PyTorch language model\n",
        "    tokenizer: SentencePiece tokenizer\n",
        "    temp: Temperature parameter; lower: more conservative, higher: more diverse\n",
        "    num_lines: No. of lines to generate.\n",
        "    max_len: Max no. of tokens per line (not words!)\n",
        "    sos_id: Start of sequence id in vocabulary\n",
        "    eos_id: End of sequence id in vocabulary\n",
        "  \n",
        "  Returns: list of strings\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  \n",
        "  lines = []\n",
        "  line_cnt = 0\n",
        "  hidden = model.init_hidden(1)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    while line_cnt != num_lines:\n",
        "      try:\n",
        "\n",
        "        ids, hidden = generate_line(model, hidden=hidden, temp=temp, max_len=max_len,\n",
        "                                    sos_id=sos_id, eos_id=eos_id, unk_id=unk_id)\n",
        "        \n",
        "        if len(ids) <= min_len: raise Exception\n",
        "        line = tokenizer.textify(ids).strip()\n",
        "        \n",
        "        if line.startswith(tuple(\"-?!.,()\")): raise Exception\n",
        "        if is_unbalanced(line): raise Exception\n",
        "        \n",
        "        lines += [line]\n",
        "        line_cnt +=1\n",
        "        \n",
        "      except Exception as e:\n",
        "        pass\n",
        "    \n",
        "  last_line = lines.pop()\n",
        "  l = parse_last_line(last_line)\n",
        "  lines.append(l)\n",
        "  \n",
        "  return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73w_4CMz-Mrv",
        "colab_type": "code",
        "outputId": "0e3e057d-5e85-4fe5-c3b8-021108d4f4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.5, num_lines=10, min_len=8, max_len=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Elmondá az útat, egészen Budáig:',\n",
              " 't, ha még most is elcsutak',\n",
              " 'vén, hogy ezt meg nem adja,',\n",
              " 'én; ha nem lesz ez, hogy kid',\n",
              " 'ünk - azaz hogy nem hitte senki;',\n",
              " 'ünk, ha majd csak egy-néha igaz,',\n",
              " 'Nosza, Károly járatlan hangi és fűzött',\n",
              " 'Szellem volt, mely a csatatér semmi szemet',\n",
              " 'Ezt hallván a táborhez félre,',\n",
              " 'ét a bokronk Doronghynipjára?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "9mCMMjXJQOQ-",
        "colab_type": "code",
        "outputId": "eb7fa399-4c34-4c19-c2d1-48a89dcf42e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.6, num_lines=10, min_len=8, max_len=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Megrezzent a leány eme hideg kéztől,',\n",
              " 'Werner, szája, az anyja, az elás',\n",
              " 'ármányaitól visszatéren vens',\n",
              " 'Mi az istennyila, féltvén az özvegynek,',\n",
              " ': s az apró evóik tan-',\n",
              " 'en a késő király, a vén Bence.',\n",
              " 'a nép, ha volna elcseitták',\n",
              " 'aféle nemzet, ti a tenger nép.',\n",
              " 'vagy csak nehány a halottak nem úgy, mint egy',\n",
              " 'a kocsihoz, gerézdét?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "_yNUM_HPSpvw",
        "colab_type": "code",
        "outputId": "d6dbea34-ab41-49ee-c243-7d3f4955b941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, temp=0.7, num_lines=10, min_len=8, max_len=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"mondok, vón' a nagy szél-tor\",\n",
              " 'Erántadék: nincs, korussa',\n",
              " 'ad, ha nem is, mi a nemcsak',\n",
              " 'yuram, Egy szem a magyar ember,',\n",
              " 'a két nép gőzilesz... vagy -',\n",
              " 'Úgy szólott ahozról, ki a minapi',\n",
              " 'adék, s vele lőn a mai párság',\n",
              " 'a terem eluntig jó minapi.',\n",
              " 'ig . Odaták be a .',\n",
              " 'égen, ki a repűlést használtatott.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}